{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf06c56-f59d-4fd5-9b56-41e0cef8c24a",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Tema 5: Aprendizaje por Refuerzo</h1>\n",
    "    <h1>Q-Learning</h1>\n",
    "    <h1></h1>\n",
    "    <h5>Prof. Wladimir Rodriguez</h5>\n",
    "    <h5>wladimir@ula.ve</h5>\n",
    "    <h5>Departamento de Computación</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69aee51-3380-4b99-b5a8-b246ddfda090",
   "metadata": {},
   "source": [
    "`Q-Learning` es un algoritmo de aprendizaje por refuerzo sin modelo para aprender el valor de una acción en un estado particular. No requiere un modelo del ambiente (por lo tanto, \"sin modelo\"), y puede manejar problemas con transiciones estocásticas y recompensas sin requerir adaptaciones.\n",
    "\n",
    "Para cualquier proceso de decisión de Markov finito (PDMF), `Q-Learning` encuentra una política óptima en el sentido de maximizar el valor esperado de la recompensa total en todos y cada uno de los pasos sucesivos, comenzando desde el estado actual. `Q-Learning` puede identificar una política de selección de acciones óptima para cualquier PDMF dado, con un tiempo de exploración infinito y una política parcialmente aleatoria.`Q` se refiere a la función que calcula el algoritmo: las recompensas esperadas por una acción realizada en un estado determinado.\n",
    "\n",
    "- El `Q-Learning` es el algoritmo Aprendizaje por Refuerzo que:\n",
    "   - Entrena una `función Q`, que contiene, como memoria interna, una `tabla Q` la cual contiene todos los valores del par estado-acción.\n",
    "    \n",
    "   - Dado un estado y una acción, nuestra `función Q` buscará en su tabla Q el valor correspondiente.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/tablaQ.png'/>\n",
    "</center>\n",
    "\n",
    "  - Cuando finaliza el entrenamiento, tenemos una `función Q` óptima, por lo tanto, una `tabla Q` óptima.\n",
    "    \n",
    "- Y si tenemos una `función Q` óptima, tenemos una política óptima, ya que sabemos para cada estado, cuál es la mejor acción a tomar.\n",
    "\n",
    "$$\\pi^*(s) = \\underset{a}{argmaxQ^*}(s,a)$$\n",
    "\n",
    "Pero, al principio, nuestra `tabla Q` es inútil, ya que da un valor arbitrario para cada par estado-acción (la mayoría de las veces inicializamos la `tabla q` con 0). Pero, a medida que exploremos el ambiente y actualicemos nuestra `tabla Q`, nos dará mejores y mejores aproximaciones.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/frozenlakeQ.png'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591775a-76b3-4fbe-9fbb-ff35207ce1fb",
   "metadata": {},
   "source": [
    "## Ejemplo de `Q-Learning` utilizando el ambiente `Frozen Lake` de la librería `Gym'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30c320-9c41-4bf2-8a11-1ffd0bee250a",
   "metadata": {},
   "source": [
    "`Frozen Lake` es un ambiente simple compuesto por mosaicos, donde el agente tiene que pasar de un mosaico inicial a uno objetivo. Los mosaicos pueden ser un lago congelado seguro o un agujero que te atrapa para siempre. El agente, tiene 4 acciones posibles: ir  a la IZQUIERDA, a ABAJO, a la DERECHA o ARRIBA. El agente debe aprender a sortear los agujeros para llegar a la meta en un número mínimo de acciones. De forma predeterminada, el ambiente siempre tiene la misma configuración. En el código del ambiente, cada mosaico está representado por una letra de la siguiente manera\n",
    "\n",
    "```\n",
    "S F F F       (S: punto de entrada, seguro)\n",
    "F H F H       (F: superficie congelada, seguro)\n",
    "F F F H       (H: hueco, atrapado para siempre)\n",
    "H F F G       (G: meta, seguro)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7d65d-f458-49c8-b076-5f5920f5cb56",
   "metadata": {},
   "source": [
    "### Importar dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9de629c8-70a1-45c1-b89d-3d897004600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a940141-4fe5-4d1f-91ca-6ce9f88e2639",
   "metadata": {},
   "source": [
    "### Crear ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e82e04a-4fa3-422e-a2d5-7fb4d0ec7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiente = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode='human')\n",
    "ambiente.reset()\n",
    "ambiente.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b1fe9-bbf9-463f-a7f6-92b4e2d51a61",
   "metadata": {},
   "source": [
    "En `Frozen Lake` hay 16 mosaicos, lo que significa que nuestro agente se puede encontrar en 16 posiciones diferentes, llamadas estados. Para cada estado, hay 4 acciones posibles: ir ◀️IZQUIERDA, 🔽ABAJO, ▶️DERECHA y 🔼ARRIBA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da272798-3d15-448b-9530-6a77c61b760f",
   "metadata": {},
   "source": [
    "#### Espacio de Estados/Observaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed42205-5511-4c43-bb0d-504c204e71bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____ESPACIO DE OBSERVACIONES_____ \n",
      "\n",
      "Forma del Espacio de Observaciones Discrete(16)\n",
      "Ejemplo de Observación 1\n"
     ]
    }
   ],
   "source": [
    "ambiente.reset()\n",
    "print(\"_____ESPACIO DE OBSERVACIONES_____ \\n\")\n",
    "print(\"Forma del Espacio de Observaciones\", ambiente.observation_space)\n",
    "print(\"Ejemplo de Observación\", ambiente.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a05714-0dc8-402e-ada9-02cfbe02edc3",
   "metadata": {},
   "source": [
    "Vemos con `Forma del Espacio de Observaciones(16)` que la observación es un valor que representa la posición actual del **agente como fila_actual * numero_fila + columna_actual (donde tanto la fila como la columna comienzan en 0)**.\n",
    "\n",
    "Por ejemplo, la posición del objetivo en el mapa 4x4 se puede calcular de la siguiente manera: 3 * 4 + 3 = 15. El número de observaciones posibles depende del tamaño del mapa. **Por ejemplo, el mapa 4x4 tiene 16 posibles observaciones.**\n",
    "\n",
    "\n",
    "Por ejemplo, así es como se ve estado = 0:\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/frozenlake.png'/>\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667842bc-acca-460c-b006-9d24f157e230",
   "metadata": {},
   "source": [
    "#### Espacio de Acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e0c923-9097-4703-872f-66ead8a42c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ESPACIO DE ACCIONES_____ \n",
      "\n",
      "Forma del Espacio de Acciones 4\n",
      "Ejemplo de Acción 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ESPACIO DE ACCIONES_____ \\n\")\n",
    "print(\"Forma del Espacio de Acciones\", ambiente.action_space.n)\n",
    "print(\"Ejemplo de Acción\", ambiente.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540d450-2355-4e8b-8d35-7cebe6f4ce0a",
   "metadata": {},
   "source": [
    "El espacio de acción (el conjunto de acciones posibles que puede realizar el agente) es discreto con 4 acciones disponibles:\n",
    "- 0: IR A LA IZQUIERDA\n",
    "- 1: ABAJO\n",
    "- 2: IR A LA DERECHA\n",
    "- 3: ARRIBA\n",
    "\n",
    "Función de recompensa:\n",
    "- Alcanzar la meta: +1\n",
    "- Alcanzar hoyo: 0\n",
    "- Alcanzar congelado: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4f9a8fb-faca-43ff-8bc9-6e326b0bb819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existen  16  posibles estados\n",
      "Existen  4  posibles acciones\n"
     ]
    }
   ],
   "source": [
    "espacio_de_estados = ambiente.observation_space.n\n",
    "print(\"Existen \", espacio_de_estados, \" posibles estados\")\n",
    "\n",
    "espacio_de_accion = ambiente.action_space.n\n",
    "print(\"Existen \", espacio_de_accion, \" posibles acciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d746033a-6ea7-4b38-a38e-03e22db1ae03",
   "metadata": {},
   "source": [
    "#### Crear la `tabla Q` y llenarla con ceros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69863146-c83b-4284-a375-b8d1a1eaf43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializar_tabla_q(espacio_de_estados, espacio_de_accion):\n",
    "  tablaQ = np.zeros((espacio_de_estados, espacio_de_accion))\n",
    "  return tablaQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fea2e13-cde5-4356-86cf-053a258fcb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozenlake_tablaQ = inicializar_tabla_q(espacio_de_estados, espacio_de_accion)\n",
    "frozenlake_tablaQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c13f8-6b3e-471f-a0c7-e9e5d9dce5f4",
   "metadata": {},
   "source": [
    "### Definir la Política `Epsilon-Avara`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a0156-75c3-4860-b08d-03a21fa0632c",
   "metadata": {},
   "source": [
    "`Epsilon-Avara` es la política de entrenamiento que maneja el compromiso entre exploración/explotación.\n",
    "\n",
    "La idea con `Epsilon Avara`:\n",
    "\n",
    "Con probabilidad $1 - \\epsilon$: hacemos explotación (es decir, nuestro agente selecciona la acción con el valor de par estado-acción más alto).\n",
    "\n",
    "Con probabilidad $\\epsilon$: hacemos exploración (intentando acciones aleatorias).\n",
    "\n",
    "Y a medida que avanza el entrenamiento vamos reduciendo progresivamente el valor de épsilon ya que cada vez necesitaremos menos exploración y más explotación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e63fc0eb-d6ca-47e7-81d6-d52e023731d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def politica_epsilon_avara(tablaQ, estado, epsilon):\n",
    "  # Generar un número aleatorio entre 0 y 1\n",
    "  numero_aleatorio = random.uniform(0,1)\n",
    "  # si numero_aleatorio > mayor que epsilon --> explotación\n",
    "  if numero_aleatorio > epsilon:\n",
    "    # Tomar la acción con el valor mayor dado el estado\n",
    "    accion = np.argmax(tablaQ[estado])\n",
    "  # else --> exploración\n",
    "  else:\n",
    "    accion = ambiente.action_space.sample()\n",
    "  \n",
    "  return accion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdd0f3b-b3eb-4a1e-a229-527156e3c5b2",
   "metadata": {},
   "source": [
    "### Definir los hiperparámetros\n",
    "\n",
    "Los hiperparámetros relacionados con la exploración son algunos de los más importantes.\n",
    "\n",
    "- Necesitamos asegurarnos de que nuestro agente **explore lo suficiente el espacio de estado** para aprender una buena aproximación de valor, para hacer eso necesitamos tener un decaimiento progresivo del épsilon.\n",
    "- Si disminuye el épsilon demasiado rápido (tasa de decaimiento demasiado alta), **corre el riesgo de que su agente se quede atascado**, ya que su agente no exploró lo suficiente el espacio de estado y, por lo tanto, no puede resolver el problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "165c8261-f1cd-442b-9597-026ac91243bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de Entrenamientos\n",
    "n_episodios_entrenamiento = 100000  # Total de episodios de entrenamiento\n",
    "tasa_de_aprendizaje = 0.7          # Tasa de aprendizaje\n",
    "\n",
    "# Parámetros de Evaluación\n",
    "n_episodios_evaluacion = 100       # Total de episodios de prueba\n",
    "\n",
    "# Parámetros del Ambiente\n",
    "nombre_ambiente = \"FrozenLake-v1\" # Nombre del ambiente\n",
    "max_pasos = 99                    # Máximo número de pasos por episodio\n",
    "gamma = 0.95                      # Tasa de Descuento\n",
    "semilla_evaluacion = []           # Semilla de evaluacion del ambiente\n",
    "\n",
    "# Parámetros Exploración\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05            # Minimum exploration probability \n",
    "tasa_decaimiento = 0.0005   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbaa95f-f3ad-42b7-8f46-9b47516d494c",
   "metadata": {},
   "source": [
    "#### Crear la funcion de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1416d53a-8635-4af2-bd23-c3299c3a8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento(n_episodios_entrenamiento, min_epsilon, max_epsilon, tasa_decaimiento, ambiente, max_pasos, tablaQ):\n",
    "  for episodio in range(n_episodios_entrenamiento):\n",
    "    # Reducir epsilon (porque necesitamos menos y menos exploración)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-tasa_decaimiento*episodio)\n",
    "    # Reiniciar el ambiente\n",
    "    estado = ambiente.reset()\n",
    "    estado = estado[0]\n",
    "    paso = 0\n",
    "    listo = False\n",
    "\n",
    "    for paso in range(max_pasos):\n",
    "      # Seleccionar la acción usando la epsilon de politica avara\n",
    "      accion = politica_epsilon_avara(tablaQ, estado, epsilon)\n",
    "\n",
    "      # Tomar la acción y observar el nuevo estado  y la recompensa\n",
    "      nuevo_estado, recompensa, listo, truncado, info = ambiente.step(accion)\n",
    "\n",
    "      # Actualizar Q(s,a):= Q(s,a) + lr * [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      tablaQ[estado][accion] = tablaQ[estado][accion] + tasa_de_aprendizaje * (recompensa + gamma * np.max(tablaQ[nuevo_estado]) - tablaQ[estado][accion])   \n",
    "\n",
    "      # Si listo, terminar el episodio\n",
    "      if listo:\n",
    "        break\n",
    "      \n",
    "      # Nuestro estadp es el nuevo estado\n",
    "      estado = nuevo_estado\n",
    "  return tablaQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6da358-e029-4471-98ef-275a88d7d0ea",
   "metadata": {},
   "source": [
    "#### Entrenar el agente `Q Learning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7620acf-a2ac-43ff-9f06-2c4e792d9122",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m frozenlake_tablaQ \u001b[38;5;241m=\u001b[39m \u001b[43mentrenamiento\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episodios_entrenamiento\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasa_decaimiento\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiente\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pasos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrozenlake_tablaQ\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m, in \u001b[0;36mentrenamiento\u001b[0;34m(n_episodios_entrenamiento, min_epsilon, max_epsilon, tasa_decaimiento, ambiente, max_pasos, tablaQ)\u001b[0m\n\u001b[1;32m     13\u001b[0m accion \u001b[38;5;241m=\u001b[39m politica_epsilon_avara(tablaQ, estado, epsilon)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Tomar la acción y observar el nuevo estado  y la recompensa\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m nuevo_estado, recompensa, listo, truncado, info \u001b[38;5;241m=\u001b[39m \u001b[43mambiente\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Actualizar Q(s,a):= Q(s,a) + lr * [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m tablaQ[estado][accion] \u001b[38;5;241m=\u001b[39m tablaQ[estado][accion] \u001b[38;5;241m+\u001b[39m tasa_de_aprendizaje \u001b[38;5;241m*\u001b[39m (recompensa \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(tablaQ[nuevo_estado]) \u001b[38;5;241m-\u001b[39m tablaQ[estado][accion])   \n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/gym/envs/toy_text/frozen_lake.py:252\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m a\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mint\u001b[39m(s), r, t, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: p})\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/gym/envs/toy_text/frozen_lake.py:273\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/gym/envs/toy_text/frozen_lake.py:367\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    365\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    366\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    370\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    371\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frozenlake_tablaQ = entrenamiento(n_episodios_entrenamiento, min_epsilon, max_epsilon, tasa_decaimiento, ambiente, max_pasos, frozenlake_tablaQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9308be2-49de-4595-a314-179bd185b19e",
   "metadata": {},
   "source": [
    "#### Observar la `tabla Q` resultante del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "beb7767f-bec9-4a14-b8ba-a136bbfafbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.26493754e-01, 9.68281473e-02, 2.00996203e-01, 1.10319698e-01],\n",
       "       [3.80784492e-02, 4.09887606e-03, 1.93945411e-02, 1.84565503e-01],\n",
       "       [3.69621490e-02, 4.47967051e-02, 3.76096958e-02, 7.62097664e-02],\n",
       "       [3.11327746e-02, 1.38608398e-02, 3.98902721e-02, 5.47352183e-02],\n",
       "       [1.71422664e-01, 5.41767800e-02, 5.64236196e-02, 5.15562463e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [5.76428864e-02, 1.19136825e-07, 2.13787318e-03, 3.74463995e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.65385754e-02, 1.06708780e-01, 5.44332374e-02, 4.14419305e-01],\n",
       "       [4.69036428e-02, 7.37766391e-01, 9.51532052e-02, 1.21571473e-02],\n",
       "       [6.67931543e-01, 3.02143165e-02, 6.86178209e-03, 1.72628332e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.37652509e-03, 5.27650906e-02, 8.27281626e-01, 3.67480120e-01],\n",
       "       [6.87458746e-01, 9.69657206e-01, 2.86161898e-01, 1.69681475e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozenlake_tablaQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b49bcc8-4023-432a-b18f-74714262253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frozenlake_tablaQ = np.array([[2.26493754e-01, 9.68281473e-02, 2.00996203e-01, 1.10319698e-01],\n",
    "       [3.80784492e-02, 4.09887606e-03, 1.93945411e-02, 1.84565503e-01],\n",
    "       [3.69621490e-02, 4.47967051e-02, 3.76096958e-02, 7.62097664e-02],\n",
    "       [3.11327746e-02, 1.38608398e-02, 3.98902721e-02, 5.47352183e-02],\n",
    "       [1.71422664e-01, 5.41767800e-02, 5.64236196e-02, 5.15562463e-02],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [5.76428864e-02, 1.19136825e-07, 2.13787318e-03, 3.74463995e-03],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [4.65385754e-02, 1.06708780e-01, 5.44332374e-02, 4.14419305e-01],\n",
    "       [4.69036428e-02, 7.37766391e-01, 9.51532052e-02, 1.21571473e-02],\n",
    "       [6.67931543e-01, 3.02143165e-02, 6.86178209e-03, 1.72628332e-03],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [1.37652509e-03, 5.27650906e-02, 8.27281626e-01, 3.67480120e-01],\n",
    "       [6.87458746e-01, 9.69657206e-01, 2.86161898e-01, 1.69681475e-01],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "135b6b53-0f7c-4ba6-a7dc-9124d48d9edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.226, 0.097, 0.201, 0.11 ],\n",
       "       [0.038, 0.004, 0.019, 0.185],\n",
       "       [0.037, 0.045, 0.038, 0.076],\n",
       "       [0.031, 0.014, 0.04 , 0.055],\n",
       "       [0.171, 0.054, 0.056, 0.052],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.058, 0.   , 0.002, 0.004],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.047, 0.107, 0.054, 0.414],\n",
       "       [0.047, 0.738, 0.095, 0.012],\n",
       "       [0.668, 0.03 , 0.007, 0.002],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.001, 0.053, 0.827, 0.367],\n",
       "       [0.687, 0.97 , 0.286, 0.17 ],\n",
       "       [0.   , 0.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozenlake_tablaQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a02870-3f92-4d39-82af-a3a65281ba03",
   "metadata": {},
   "source": [
    "#### Definir la función de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "925be64f-a0fb-4a90-9a86-7d0202a9b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_agente(ambiente, max_pasos, n_episodios_evaluacion, Q, semilla):\n",
    "\n",
    "  recompensa_episodio = []\n",
    "  for episode in range(n_episodios_evaluacion):\n",
    "    if semilla:\n",
    "      estado = ambiente.reset(seed=semilla[episodio])\n",
    "      estado = estado[0]\n",
    "    else:\n",
    "      estado = ambiente.reset()\n",
    "      estado = estado[0]\n",
    "    paso = 0\n",
    "    listo = False\n",
    "    recompensa_total_episodio = 0\n",
    "    \n",
    "    for paso in range(max_pasos):\n",
    "      # Take the action (index) that have the maximum expected future reward given that state\n",
    "      accion = np.argmax(Q[estado][:])\n",
    "      nuevo_estado, recompensa, listo, truncado, info = ambiente.step(accion)\n",
    "      recompensa_total_episodio += recompensa\n",
    "      if n_episodios_evaluacion == 1:\n",
    "          print(nuevo_estado)  \n",
    "      if listo:\n",
    "        break\n",
    "      estado = nuevo_estado\n",
    "    recompensa_episodio.append(recompensa_total_episodio)\n",
    "    print(episode)  \n",
    "  recompensa_media = np.mean(recompensa_episodio)\n",
    "  recompensa_desviacion_estandar = np.std(recompensa_episodio)\n",
    "\n",
    "  return recompensa_media, recompensa_desviacion_estandar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27380862-147f-4dc3-af50-86bd7b03cfea",
   "metadata": {},
   "source": [
    "### Evaluar a nuestro agente `Q-Learning` \n",
    "\n",
    "- Normalmente deberías tener una recompensa media de 1.0\n",
    "- Es relativamente fácil ya que el espacio de estado es realmente pequeño (16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74cf9c90-c3fc-4238-9206-d925e0ca8394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Recompensa media=0.68 +/- 0.47\n"
     ]
    }
   ],
   "source": [
    "recompensa_media, recompensa_desviacion_estandar = evaluar_agente(ambiente, max_pasos, n_episodios_evaluacion, frozenlake_tablaQ, semilla_evaluacion)\n",
    "print(f\"Recompensa media={recompensa_media:.2f} +/- {recompensa_desviacion_estandar:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b274c5ba-7e6f-4e7d-841a-c379fdd1f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "recompensa_media, recompensa_desviacion_estandar = evaluar_agente(ambiente, max_pasos, 1, frozenlake_tablaQ, semilla_evaluacion)\n",
    "print(f\"Recompensa media={recompensa_media:.2f} +/- {recompensa_desviacion_estandar:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e8c52-604c-4602-ac10-1c7872326621",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='../figuras/frozenlakeRun.png'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f840d96-a43c-4723-bfef-d7d90fd74d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
