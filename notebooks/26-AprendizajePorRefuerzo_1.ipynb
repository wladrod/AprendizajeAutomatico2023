{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Tema 5: Aprendizaje por Refuerzo</h1>\n",
    "    <h1>Introducci√≥n al Aprendizaje por Refuerzo</h1>\n",
    "    <h1></h1>\n",
    "    <h5>Prof. Wladimir Rodriguez</h5>\n",
    "    <h5>wladimir@ula.ve</h5>\n",
    "    <h5>Departamento de Computaci√≥n</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por Refuerzo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "<center>\n",
    "<img src='../figuras/Agente_Entorno.png' width=\"50%\"/>\n",
    "</center>\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "El aprendizaje por refuerzo es aprender qu√© hacer, dada una situaci√≥n y un conjunto de posibles acciones para elegir, con el fin de maximizar una recompensa. Al alumno, que llamaremos agente, no se le dice qu√© hacer, debe descubrirlo por s√≠ mismo a trav√©s de la interacci√≥n con el ambienta. El objetivo es elegir sus acciones de tal manera que la recompensa acumulada se maximice. Entonces, elegir la mejor recompensa ahora, podr√≠a no ser la mejor decisi√≥n, a la larga.\n",
    "\n",
    "- Una definici√≥n formal\n",
    "\n",
    "> El aprendizaje por refuerzo es un marco para resolver tareas de control (tambi√©n llamados problemas de decisi√≥n) mediante la creaci√≥n de agentes que aprenden del ambiente interactuando con √©l a trav√©s de prueba y error y recibiendo recompensas (positivas o negativas) como su √∫nica retroalimentaci√≥n.\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "<img src='../figuras/TiposAprendizaje.png' width=\"75%\"/>\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "El aprendizaje por refuerzo es diferente del aprendizaje supervisado, el tipo de aprendizaje estudiado\n",
    "en la mayor√≠a de las aplicaciones en el campo del aprendizaje autom√°tico. El aprendizaje supervisado es\n",
    "aprender de un conjunto de entrenamiento de ejemplos etiquetados proporcionados por un supervisor externo.\n",
    "Cada ejemplo es una descripci√≥n de una situaci√≥n junto con una especificaci√≥n, la etiqueta, de la acci√≥n correcta que el sistema deber√≠a tomar en esa situaci√≥n, que a menudo es identificar la categor√≠a a la que pertenece la situaci√≥n El objetivo de este tipo de aprendizaje es que el sistema extrapole o generalice sus respuestas para que act√∫e correctamente en situaciones que no est√°n presentes en el conjunto de entrenamiento. Este es un tipo importante de aprendizaje, pero por si solo no es adecuado para aprender de la interacci√≥n. En problemas interactivos, a menudo no es pr√°ctico obtener ejemplos del comportamiento deseados que sean correctos y representativos de todas las situaciones en las que el agente tiene que actuar. En territorio inexplorado, donde uno esperar√≠a que el aprendizaje sea m√°s beneficioso, un agente debe ser capaz de aprender de su propia experiencia.\n",
    "\n",
    "El aprendizaje por refuerzo tambi√©n es diferente del aprendizaje no supervisado, que generalmente trata de encontrar la estructura oculta en colecciones de datos sin etiqueta. Los t√©rminos aprendizaje supervisado y aprendizaje no supervisado parecer√≠an clasificar exhaustivamente los paradigmas de aprendizaje autom√°tico, pero no es as√≠. Aunque uno podr√≠a estar tentado a pensar en el aprendizaje por refuerzo como una especie de aprendizaje no supervisado porque no se basa en ejemplos de comportamiento correcto, el aprendizaje por refuerzo est√° intentando maximizar una se√±al de recompensa en lugar de tratar de encontrar la estructura oculta. Descubriendo la estructura en la experiencia de un agente ciertamente puede ser √∫til en el aprendizaje por refuerzo, pero por s√≠ mismo no aborda el problema de aprendizaje por resfuerzo de maximizar la se√±al de recompensa.\n",
    "\n",
    "Por lo tanto, se puede considerar que el aprendizaje por refuerzo es un tercer paradigma de aprendizaje autom√°tico, junto con el aprendizaje supervisado y el aprendizaje no supervisado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de Aprendizaje por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Balance Carro-Poste\n",
    "\n",
    "<img src='../figuras/Car_Pole_Balancing.png' width='50%' />\n",
    "\n",
    "- **Objetivo**: equilibrar el poste sobre un carro en movimiento\n",
    "- **Estado**: √°ngulo, velocidad angular, posici√≥n, velocidad horizontal\n",
    "- **Acciones**: fuerza horizontal al carro\n",
    "- **Recompensa**: 1 en cada paso de tiempo si el poste est√° en posici√≥n vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Juegos de Atari\n",
    "\n",
    "#### Breakout\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"../figuras/atari.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "- **Objetivo**: Ganar el juego con el puntaje m√°s alto\n",
    "- **Estado**: P√≠xeles de la pantalla del juego\n",
    "- **Acciones**: arriba, abajo, izquierda, derecha, etc.\n",
    "- **Recompensa** - Puntuaci√≥n proporcionada por el juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pac-Mac\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"../figuras/PacMac.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "https://github.com/tychovdo/PacmanDQN\n",
    "\n",
    "- **Objetivo**: Ganar el juego con el puntaje m√°s alto\n",
    "- **Estado**: P√≠xeles de la pantalla del juego\n",
    "- **Acciones**: arriba, abajo, izquierda, derecha, etc.\n",
    "- **Recompensa** - Puntuaci√≥n proporcionada por el juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entrenar Robots para el embalaje \n",
    "\n",
    "<img src='../figuras/Robot.png' width=\"50%\" />\n",
    "\n",
    "- **Objetivo**: Elegir un dispositivo de una caja y ponerlo en un contenedor\n",
    "- **Estado**: P√≠xeles brutos del mundo real\n",
    "- **Acciones**: Posibles acciones del robot\n",
    "- **Recompensa**: Positiva al colocar un dispositivo con √©xito; de lo contrario, negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La hip√≥tesis de la recompensa: la idea central del Aprendizaje por Refuerzo\n",
    "\n",
    "¬øPor qu√© el objetivo del agente es maximizar el rendimiento esperado?\n",
    "\n",
    "Porque el Aprendizaje por Refuerzo se basa en la hip√≥tesis de la recompensa, que es que todos los objetivos pueden describirse como la maximizaci√≥n del rendimiento esperado (recompensa acumulada esperada).\n",
    "\n",
    "Es por eso que en el Aprendizaje por Refuerzo, para tener el mejor comportamiento, necesitamos maximizar la recompensa acumulada esperada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalizando el problema del Aprendizaje por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Proceso de Decisi√≥n de Markov (MDP por sus siglas en ingl√©s) es una formulaci√≥n matem√°tica del problema del Aprendizaje por Refuerzo. Que satisfacen la propiedad de Markov:\n",
    "\n",
    "**Propiedad de Markov**: el estado actual representa por completo el estado del ambiente (mundo). Es decir, el futuro depende solo del presente.\n",
    "\n",
    "Un MDP puede definirse por (S, A, R, P, Œ≥) donde:\n",
    "\n",
    "- **S**: conjunto de posibles estados\n",
    "- **A**: conjunto de posibles acciones\n",
    "- **R**: distribuci√≥n de la probabilidad de la recompensa dado el par (estado, acci√≥n)\n",
    "- **P**: distribuci√≥n de probabilidad de que tan posible que alguno de los estados sea el nuevo estado, dado el par (estado, acci√≥n). Tambi√©n conocido como probabilidad de transici√≥n.\n",
    "- **$\\gamma$**: factor de descuento de la recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Espacio de Observaciones/Estados\n",
    "\n",
    "Las observaciones/estados son la informaci√≥n que nuestro agente obtiene del entorno. En el caso de un videojuego, puede ser un fotograma (una captura de pantalla). En el caso del agente comercial, puede ser el valor de una determinada acci√≥n, etc.\n",
    "\n",
    "Hay que hacer una diferenciaci√≥n entre observaci√≥n y estado:\n",
    "\n",
    "- Estado $s$: es una descripci√≥n completa del estado del mundo (no hay informaci√≥n oculta). En un entorno completamente observado.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/chess.jpg'/>\n",
    "</center>\n",
    "\n",
    "En el juego de ajedrez, recibimos un estado del entorno ya que tenemos acceso a toda la informaci√≥n del tablero. Por esto en un juego de ajedrez, estamos en un entorno completamente observado.\n",
    "\n",
    "- Observaci√≥n $o$: es una descripci√≥n parcial del estado. En un entorno parcialmente observado.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/mario.jpg'/>\n",
    "</center>\n",
    "\n",
    "En Super Mario Bros, estamos en un entorno parcialmente observado, solo vemos una parte del nivel cerca del jugador, por lo que recibimos una observaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Espacio de acci√≥n\n",
    "\n",
    "El espacio de acci√≥n es el conjunto de todas las acciones posibles en un entorno. Las acciones pueden provenir de un espacio discreto o continuo:\n",
    "\n",
    "- *Espacio discreto*: el n√∫mero de acciones posibles es finito.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/mario.jpg'/>\n",
    "</center>\n",
    "\n",
    "En Super Mario Bros, tenemos un conjunto finito de acciones ya que solo tenemos 4 direcciones y salto.\n",
    "\n",
    "- *Espacio continuo*: el n√∫mero de acciones posibles es infinito.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/self_driving_car.jpeg'/>\n",
    "</center>\n",
    "\n",
    "Un agente de Carro Aut√≥nomo tiene infinidad de acciones posibles ya que puede girar 20¬∞ a la izquierda, 21,1¬∞, 21,2¬∞, tocar la bocina, girar 20¬∞ a la derecha‚Ä¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompensas y descuentos\n",
    "\n",
    "La recompensa es fundamental en RL porque es la √∫nica retroalimentaci√≥n para el agente. Gracias a ella, nuestro agente sabe si la acci√≥n realizada fue buena o no.\n",
    "\n",
    "La recompensa acumulada en cada paso de tiempo t se puede escribir como:\n",
    "\n",
    "$$R(\\tau)=\\sum_{t\\ge 0}^\\infty r_t$$\n",
    "\n",
    "$$R_t = r_t + r_{t+1} + \\dots + r_n$$\n",
    "\n",
    "Sin embargo, en realidad, no podemos simplemente agregarlos as√≠. Las recompensas que llegan antes (al comienzo del juego) tienen m√°s probabilidades de suceder, ya que son m√°s predecibles que las recompensas futuras a largo plazo. Por lo que se descontaran las recompenzas a largo plazo \n",
    "\n",
    "Para descontar las recompensas, procedemos as√≠:\n",
    "\n",
    "- Definimos una tasa de descuento llamada gamma ($\\gamma$). Debe estar entre 0 y 1. La mayor√≠a de las veces entre 0,99 y 0,95.\n",
    "- Cuanto mayor sea la gamma, menor ser√° el descuento. Esto significa que nuestro agente se preocupa m√°s por la recompensa a largo plazo.\n",
    "- Por otro lado, cuanto menor sea la gamma, mayor ser√° el descuento. Esto significa que nuestro agente se preocupa m√°s por la recompensa a corto plazo.\n",
    "- Luego, cada recompensa ser√° descontada por gamma al exponente del paso de tiempo, por lo que la recompensa futura es cada vez menos probable.\n",
    "\n",
    "$$R(\\tau)=\\sum_{t\\ge 0}^\\infty \\gamma^k r_t$$\n",
    "\n",
    "$$R_t = r_t + {\\gamma}r_{t+1} + \\dots + \\gamma^{n-t}r_n = r_t+{\\gamma}R_{t+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipo de tareas\n",
    "\n",
    "Una tarea es una instancia de un problema de aprendizaje por refuerzo. Podemos tener dos tipos de tareas: epis√≥dicas y continuas.\n",
    "\n",
    "\n",
    "- *Tarea epis√≥dica*: En este caso, tenemos un punto de partida y un punto final (un estado terminal). Esto crea un episodio: una lista de Estados, Acciones, Recompensas y nuevos Estados. Por ejemplo, piensa en Super Mario Bros: un episodio comienza con el lanzamiento de un nuevo nivel de Mario y termina cuando te matan o llegas al final del nivel.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/mario.jpg'/>\n",
    "</center>\n",
    "\n",
    "- *Tareas continuas*: Estas son tareas que contin√∫an para siempre (sin estado terminal). En este caso, el agente debe aprender a elegir las mejores acciones y simult√°neamente interactuar con el entorno. Por ejemplo, un agente que realiza transacciones burs√°tiles automatizadas. Para esta tarea, no hay un punto de partida ni un estado terminal. El agente sigue corriendo hasta que decidimos detenerlo.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/bolsa.png'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compromiso entre exploraci√≥n/explotaci√≥n\n",
    "\n",
    "Finalmente, antes de ver los diferentes m√©todos para resolver problemas de aprendizaje por refuerzo, debemos cubrir otro tema muy importante: la compensaci√≥n de exploraci√≥n/explotaci√≥n.\n",
    "\n",
    "- La exploraci√≥n es explorar el entorno al intentar acciones aleatorias para encontrar m√°s informaci√≥n sobre el entorno.\n",
    "\n",
    "- La explotaci√≥n es explotar informaci√≥n conocida para maximizar la recompensa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existen dos enfoques principales para resolver problemas de Aprendizaje por Refuerzo\n",
    "\n",
    "Ahora que aprendimos el enfoque de Aprendizaje por Refuerzo, ¬øc√≥mo resolvemos el problema de Aprendizaje por Refuerzo?\n",
    "\n",
    "En otros t√©rminos, ¬øc√≥mo construir un agente de Aprendizaje por Refuerzo que pueda seleccionar las acciones que maximicen su recompensa acumulada esperada?\n",
    "\n",
    "\n",
    "#### La Pol√≠tica $\\pi$: el cerebro del agente\n",
    "\n",
    "La Pol√≠tica $\\pi$ es el cerebro de nuestro Agente, es la funci√≥n que nos dice qu√© acci√≥n tomar dado el estado en el que nos encontramos. Por lo que define el comportamiento del agente en un momento dado.\n",
    "\n",
    "Esta Pol√≠tica es la funci√≥n que queremos aprender, nuestro objetivo es encontrar la pol√≠tica √≥ptima $\\pi^*$, la pol√≠tica que maximiza la rentabilidad esperada cuando el agente act√∫a de acuerdo con ella. Encontramos esta $\\pi^*$ a trav√©s del entrenamiento.\n",
    "\n",
    "Hay dos enfoques para entrenar a nuestro agente para encontrar esta pol√≠tica √≥ptima $\\pi^*$:\n",
    "\n",
    "- *Directamente*, ense√±ando al agente a aprender qu√© acci√≥n tomar, dado el estado en el que se encuentra: M√©todos Basados en Pol√≠ticas.\n",
    "- *Indirectamente*, ense√±e al agente a aprender qu√© estado es m√°s valioso y luego tome la acci√≥n que lo lleve a los estados m√°s valiosos: M√©todos Basados en Valores.\n",
    "\n",
    "#### M√©todos Basados en Pol√≠ticas.\n",
    "\n",
    "En los m√©todos basados en pol√≠ticas, aprendemos una funci√≥n de pol√≠tica directamente.\n",
    "\n",
    "Esta funci√≥n mapear√° desde cada estado a la mejor acci√≥n correspondiente en ese estado. O una distribuci√≥n de probabilidad sobre el conjunto de acciones posibles en ese estado.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/policy_based.png'/>\n",
    "</center>\n",
    "\n",
    "Tenemos dos tipos de pol√≠ticas:\n",
    "\n",
    "- *Determinista*: una pol√≠tica en un estado determinado siempre devolver√° la misma acci√≥n.\n",
    "\n",
    "$$a=\\pi(s)$$\n",
    "\n",
    "- Estoc√°stica: genera una distibuci√≥n de probabilidad sobre las acciones.\n",
    "\n",
    "$$\\pi(a|s)=P[A|s]$$\n",
    "\n",
    "pol√≠tica(acciones | estado) = distribuci√≥n de probabilidad sobre el conjunto de acciones dado el estado actual\n",
    "\n",
    "#### M√©todos basados en valores\n",
    "\n",
    "En los m√©todos basados en valores, en lugar de entrenar una funci√≥n de pol√≠tica, entrenamos una funci√≥n de valor que asigna un estado al valor esperado de estar en ese estado.\n",
    "\n",
    "El valor de un estado es el rendimiento descontado esperado que el agente puede obtener si comienza en ese estado y luego act√∫a de acuerdo con nuestra pol√≠tica.\n",
    "\n",
    "\"Actuar de acuerdo con nuestra pol√≠tica\" simplemente significa que nuestra pol√≠tica es \"ir al estado con el valor m√°s alto\".\n",
    "\n",
    "$$v_\\pi(s)=\\mathbb{E}_\\pi[R_{t+1}+R_{t+2}+R_{t+3}+\\dots|S_t=s]$$\n",
    "\n",
    "Aqu√≠ vemos que nuestra funci√≥n de valor defini√≥ el valor para cada estado posible.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/value_based.png'/>\n",
    "</center>\n",
    "\n",
    "Gracias a nuestra funci√≥n de valor, en cada paso nuestra pol√≠tica seleccionar√° el estado con el mayor valor definido por la funci√≥n de valor: -7, luego -6, luego -5 (y as√≠ sucesivamente) hasta alcanzar la meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de Aprendizaje por Refuerzo\n",
    "\n",
    "En este ejemplo utilizaremos una librer√≠a para crear ambientes para el Aprendizaje por Refuerzo llamada [Gym](https://www.gymlibrary.dev).\n",
    "Gym es una librer√≠a Python de c√≥digo abierto para desarrollar y comparar algoritmos de aprendizaje por refuerzo al proporcionar una API est√°ndar para comunicar algoritmos con entornos de aprendizaje, as√≠ como un conjunto est√°ndar de entornos que cumplen con esa API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librer√≠a que contiene nuestro ambiente (entorno) se llama Gym.\n",
    "\n",
    "La biblioteca de Gym proporciona dos cosas:\n",
    "\n",
    "- Una interfaz que te permite crear ambientes de Aprendizaje por Refuerzo.\n",
    "- Una colecci√≥n de ambientes (gym-control, atari, box2D...).\n",
    "\n",
    "Con Gym:\n",
    "\n",
    "1. Creamos nuestro entorno usando `gym.make()`\n",
    "\n",
    "2. Restablecemos el entorno a su estado inicial con `observacion = env.reset()`\n",
    "\n",
    "En cada paso:\n",
    "\n",
    "3. Obt√©n una acci√≥n usando nuestro modelo (en nuestro ejemplo tomamos una acci√≥n aleatoria)\n",
    "\n",
    "4. Usando `env.step(action)`, realizamos esta acci√≥n en el entorno y obtenemos\n",
    "    - `observaci√≥n`: El nuevo estado ($s_{t+1}$)\n",
    "    - `recompenza`: La recompensa que obtenemos tras ejecutar la acci√≥n\n",
    "    - `listo`: Indica si el episodio termin√≥\n",
    "    - `info`: Un diccionario que proporciona informaci√≥n adicional (depende del ambiente).\n",
    "\n",
    "Si el episodio ha terminado:\n",
    "- Restablecemos el entorno a su estado inicial con `observation = env.reset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acci√≥n tomada: 3\n",
      "Acci√≥n tomada: 2\n",
      "Acci√≥n tomada: 2\n",
      "Acci√≥n tomada: 1\n",
      "Acci√≥n tomada: 1\n",
      "Acci√≥n tomada: 2\n",
      "Acci√≥n tomada: 1\n",
      "Acci√≥n tomada: 2\n",
      "Acci√≥n tomada: 3\n",
      "Acci√≥n tomada: 3\n",
      "Acci√≥n tomada: 3\n",
      "Acci√≥n tomada: 0\n",
      "Acci√≥n tomada: 0\n",
      "Acci√≥n tomada: 3\n",
      "Acci√≥n tomada: 1\n",
      "Acci√≥n tomada: 3\n",
      "Acci√≥n tomada: 1\n",
      "Acci√≥n tomada: 1\n",
      "Acci√≥n tomada: 1\n",
      "Acci√≥n tomada: 2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Primero, creamos un ambiente(entorno) llamado LunarLander-v2\n",
    "ambiente = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Posteriormente reiniciamos este ambiente\n",
    "observacion = ambiente.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "  # Tomar una acci√≥n al azar\n",
    "  accion = ambiente.action_space.sample()\n",
    "  print(\"Acci√≥n tomada:\", accion)\n",
    "\n",
    "  # Ejecutar la acci√≥n en el ambiente y obtener\n",
    "  # el pr√≥ximo estado, rencompensa, listo e informaci√≥n adicional\n",
    "  observacion, rencompensa, listo, truncado, info = ambiente.step(accion)\n",
    "  \n",
    "  # Si el juego finalizo (en nuestro caso, aterizamos, nos estrellamos o se acabo el tiempo)\n",
    "  if listo:\n",
    "      # Resetear el ambiente\n",
    "      print(\"Reiniciar el ambiente\")\n",
    "      observacion = ambiente.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El ambiente\n",
    "\n",
    "En este ejemplo vamos a entrenar a nuestro agente, un `Lunar Lander`, para aterrizar correctamente en la luna. Para hacer eso, el agente necesita aprender a adaptar su velocidad y posici√≥n (horizontal, vertical y angular) para aterrizar correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____ESPACIO DE OBSERVACI√ìN_____ \n",
      "\n",
      "Forma del Espacio de Observaci√≥n (8,)\n",
      "Observaci√≥n aleatoria [-0.98635596  0.9278961  -3.3600903   0.5214729  -2.5375593   3.4410696\n",
      "  0.76094836  0.8141851 ]\n"
     ]
    }
   ],
   "source": [
    "# Crear ambiente con gym.make(\"<nombre_del_ambiente>\")\n",
    "ambiente = gym.make(\"LunarLander-v2\")\n",
    "ambiente.reset()\n",
    "print(\"_____ESPACIO DE OBSERVACI√ìN_____ \\n\")\n",
    "print(\"Forma del Espacio de Observaci√≥n\", ambiente.observation_space.shape)\n",
    "print(\"Observaci√≥n aleatoria\", ambiente.observation_space.sample()) # Obtener una observaci√≥n aleatoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos con `Forma del Espacio de Observaci√≥n (8,)` que la observaci√≥n es un vector de tama√±o 8, donde cada valor contiene informaci√≥n diferente sobre el m√≥dulo de aterrizaje:\n",
    "- Coordenada horizontal de la plataforma (x)\n",
    "- Coordenada vertical de la plataforma (y)\n",
    "- Velocidad horizontal (x)\n",
    "- Velocidad vertical (y)\n",
    "- √Ångulo\n",
    "- Velocidad angular\n",
    "- Si la pierna izquierda tiene punto de contacto toc√≥ la tierra\n",
    "- Si la pierna derecha tiene punto de contacto toc√≥ la tierra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ESPACIO_DE_ACCI√ìN_____ \n",
      "\n",
      "Forma del Espacio de Acci√≥n 4\n",
      "Acci√≥n aleatoria 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ESPACIO_DE_ACCI√ìN_____ \\n\")\n",
    "print(\"Forma del Espacio de Acci√≥n\", ambiente.action_space.n)\n",
    "print(\"Acci√≥n aleatoria\", ambiente.action_space.sample()) # Obtener una acci√≥n aleatoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El espacio de acci√≥n (el conjunto de acciones posibles que puede realizar el agente) es discreto con 4 acciones disponibles:\n",
    "\n",
    "- Hacer nada,\n",
    "- Dispara motor de orientaci√≥n izquierda,\n",
    "- Dispara el motor principal,\n",
    "- Dispara motor de orientaci√≥n derecha.\n",
    "\n",
    "Funci√≥n de recompensa (la funci√≥n que otorga una recompensa en cada paso de tiempo):\n",
    "\n",
    "- Moverse desde la parte superior de la pantalla hasta la plataforma de aterrizaje y la velocidad cero es de aproximadamente 100 a 140 puntos.\n",
    "- El motor principal de disparo es -0.3 cada cuadro\n",
    "- Cada contacto con el suelo de la pierna es +10 puntos\n",
    "- El episodio termina si el m√≥dulo de aterrizaje se estrella (adicional -100 puntos) o se detiene (+100 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiente vectorizado\n",
    "\n",
    "Creamos un ambiente vectorizado (m√©todo para apilar m√∫ltiples ambientes independientes en un solo ambiente) de 16 ambientes, de esta manera, tendremos experiencias m√°s diversas durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el ambiente(entorno)\n",
    "ambiente = make_vec_env('LunarLander-v2', n_envs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el modelo\n",
    "\n",
    "- Ahora que estudiamos nuestro ambiente y entendimos el problema: **poder aterrizar correctamente el m√≥dulo de aterrizaje lunar en la plataforma de aterrizaje controlando los motores de orientaci√≥n izquierdo, derecho y principal**. Construyamos el algoritmo que vamos a usar para resolver este problema üöÄ.\n",
    "\n",
    "- Para hacerlo, vamos a utilizar la librer√≠a de Aprendizaje por Refuerzo, [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/).\n",
    "\n",
    "- SB3 es un conjunto de **implementaciones confiables de algoritmos de aprendizaje por refuerzo en PyTorch**.\n",
    "\n",
    "Para resolver este problema, vamos a utilizar SB3 **PPO**. [PPO (tambi√©n conocido como optimizaci√≥n de pol√≠tica proximal)](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).\n",
    "\n",
    "PPO es una combinaci√≥n de:\n",
    "- *M√©todo de aprendizaje por refuerzo basado en valores*: aprender una funci√≥n de acci√≥n-valor que nos indicar√° cu√°l es la **acci√≥n m√°s valiosa a realizar dado un estado y una acci√≥n**.\n",
    "- *M√©todo de aprendizaje por refuerzo basado en pol√≠ticas*: aprender una pol√≠tica que **nos dar√° una distribuci√≥n de probabilidad sobre las acciones**.\n",
    "\n",
    "Stable-Baselines3 es f√°cil de configurar:\n",
    "\n",
    "1. Crear el ambiente (en nuestro caso se hizo arriba)\n",
    "\n",
    "2. Definir el modelo que se quiere usar y crear una instancia de este modelo\n",
    "\n",
    "3. Entrenar al agente con `modelo.learn` y defines el n√∫mero de pasos de tiempo de entrenamiento\n",
    "\n",
    "```\n",
    "# Crear ambiente\n",
    "ambiente = gym.make('LunarLander-v2')\n",
    "\n",
    "# Instanciar el agente\n",
    "modelo = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Entrenar al agente\n",
    "modelo.learn(total_timesteps=int(2e5))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Crear modelo y seleccionar par√°metros para accelerar el entrenamiento\n",
    "modelo = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = ambiente,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 64,\n",
    "    n_epochs = 4,\n",
    "    gamma = 0.999,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1,\n",
    "    device='mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar al modelo (agente) de PPO üèÉ\n",
    "\n",
    "Entrenaremos a nuestro modelo para 500¬†000 intervalos de tiempo, no olvide usar GPU en Colab. Tomar√° aproximadamente ~10 minutos, pero puede usar menos intervalos de tiempo si solo desea probarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.5     |\n",
      "|    ep_rew_mean     | -187     |\n",
      "| time/              |          |\n",
      "|    fps             | 2265     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 91.4        |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 954         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007501182 |\n",
      "|    clip_fraction        | 0.0674      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.00145    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.71e+03    |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.00559    |\n",
      "|    value_loss           | 5.08e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 93           |\n",
      "|    ep_rew_mean          | -136         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 804          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054419376 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.00525     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.54e+03     |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | -0.00532     |\n",
      "|    value_loss           | 2.87e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 96           |\n",
      "|    ep_rew_mean          | -121         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 746          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067481156 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.001       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 449          |\n",
      "|    n_updates            | 12           |\n",
      "|    policy_gradient_loss | -0.00301     |\n",
      "|    value_loss           | 1.4e+03      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 94.8        |\n",
      "|    ep_rew_mean          | -97.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 715         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007519068 |\n",
      "|    clip_fraction        | 0.0885      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.000106   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 326         |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | -0.00596    |\n",
      "|    value_loss           | 590         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 106          |\n",
      "|    ep_rew_mean          | -90.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 695          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 141          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076959846 |\n",
      "|    clip_fraction        | 0.057        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | -0.000689    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 155          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00599     |\n",
      "|    value_loss           | 479          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 118         |\n",
      "|    ep_rew_mean          | -67.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 682         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010581596 |\n",
      "|    clip_fraction        | 0.0952      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | -4.53e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 287         |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.00765    |\n",
      "|    value_loss           | 565         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 117          |\n",
      "|    ep_rew_mean          | -52.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 673          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 194          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074475696 |\n",
      "|    clip_fraction        | 0.0594       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | -4.01e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 296          |\n",
      "|    n_updates            | 28           |\n",
      "|    policy_gradient_loss | -0.00416     |\n",
      "|    value_loss           | 453          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 126         |\n",
      "|    ep_rew_mean          | -47.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 511         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 288         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010510441 |\n",
      "|    clip_fraction        | 0.063       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | -2.38e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 265         |\n",
      "|    n_updates            | 32          |\n",
      "|    policy_gradient_loss | -0.00518    |\n",
      "|    value_loss           | 402         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 136        |\n",
      "|    ep_rew_mean          | -27.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 520        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 314        |\n",
      "|    total_timesteps      | 163840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00599605 |\n",
      "|    clip_fraction        | 0.0363     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.21      |\n",
      "|    explained_variance   | -1.79e-06  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 151        |\n",
      "|    n_updates            | 36         |\n",
      "|    policy_gradient_loss | -0.00192   |\n",
      "|    value_loss           | 404        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 134         |\n",
      "|    ep_rew_mean          | -22.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 527         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 341         |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007691025 |\n",
      "|    clip_fraction        | 0.033       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | -1.31e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 278         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00252    |\n",
      "|    value_loss           | 447         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 149         |\n",
      "|    ep_rew_mean          | -12.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 531         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 369         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007387871 |\n",
      "|    clip_fraction        | 0.032       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | -1.91e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 384         |\n",
      "|    n_updates            | 44          |\n",
      "|    policy_gradient_loss | -0.00173    |\n",
      "|    value_loss           | 561         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 184         |\n",
      "|    ep_rew_mean          | -8.27       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 532         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 400         |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005838329 |\n",
      "|    clip_fraction        | 0.0327      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 4.77e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 189         |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    value_loss           | 540         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 251         |\n",
      "|    ep_rew_mean          | -12.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 628         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008792279 |\n",
      "|    clip_fraction        | 0.0783      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | -4.77e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 341         |\n",
      "|    n_updates            | 52          |\n",
      "|    policy_gradient_loss | -0.00398    |\n",
      "|    value_loss           | 568         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 268         |\n",
      "|    ep_rew_mean          | -10.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 658         |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003560141 |\n",
      "|    clip_fraction        | 0.0252      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.000184    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 411         |\n",
      "|    n_updates            | 56          |\n",
      "|    policy_gradient_loss | -0.000927   |\n",
      "|    value_loss           | 578         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 318         |\n",
      "|    ep_rew_mean          | -12.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 702         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004570827 |\n",
      "|    clip_fraction        | 0.0252      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.00113     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 353         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0015     |\n",
      "|    value_loss           | 670         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 389          |\n",
      "|    ep_rew_mean          | -14.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 377          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 738          |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051891347 |\n",
      "|    clip_fraction        | 0.029        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.17        |\n",
      "|    explained_variance   | 0.0143       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 152          |\n",
      "|    n_updates            | 64           |\n",
      "|    policy_gradient_loss | -0.00278     |\n",
      "|    value_loss           | 420          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 490         |\n",
      "|    ep_rew_mean          | -10.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 774         |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004215752 |\n",
      "|    clip_fraction        | 0.014       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 70.3        |\n",
      "|    n_updates            | 68          |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 381         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 582         |\n",
      "|    ep_rew_mean          | -7.01       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 810         |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004452089 |\n",
      "|    clip_fraction        | 0.0265      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.546       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 122         |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.00252    |\n",
      "|    value_loss           | 238         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 619          |\n",
      "|    ep_rew_mean          | 4.88         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 385          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 850          |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055597886 |\n",
      "|    clip_fraction        | 0.0298       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.712        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 105          |\n",
      "|    n_updates            | 76           |\n",
      "|    policy_gradient_loss | -0.00278     |\n",
      "|    value_loss           | 174          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 680         |\n",
      "|    ep_rew_mean          | 20.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 326         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 1052        |\n",
      "|    total_timesteps      | 344064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006942802 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.4        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.003      |\n",
      "|    value_loss           | 114         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 706          |\n",
      "|    ep_rew_mean          | 37.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 1723         |\n",
      "|    total_timesteps      | 360448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066503263 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 66.6         |\n",
      "|    n_updates            | 84           |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 83.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 681         |\n",
      "|    ep_rew_mean          | 44.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 1758        |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006989425 |\n",
      "|    clip_fraction        | 0.0658      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59.6        |\n",
      "|    n_updates            | 88          |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    value_loss           | 163         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 653         |\n",
      "|    ep_rew_mean          | 49.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 219         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 1794        |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004777539 |\n",
      "|    clip_fraction        | 0.0295      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59.5        |\n",
      "|    n_updates            | 92          |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    value_loss           | 173         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 667         |\n",
      "|    ep_rew_mean          | 54.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 1836        |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006286043 |\n",
      "|    clip_fraction        | 0.0425      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.8        |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | -0.00221    |\n",
      "|    value_loss           | 130         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 685         |\n",
      "|    ep_rew_mean          | 61.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 1874        |\n",
      "|    total_timesteps      | 425984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004469488 |\n",
      "|    clip_fraction        | 0.034       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.3        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.000971   |\n",
      "|    value_loss           | 63.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 723          |\n",
      "|    ep_rew_mean          | 71.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 231          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 1909         |\n",
      "|    total_timesteps      | 442368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056932936 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.6         |\n",
      "|    n_updates            | 104          |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 759          |\n",
      "|    ep_rew_mean          | 81.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 235          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 1946         |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051294314 |\n",
      "|    clip_fraction        | 0.0357       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.62         |\n",
      "|    n_updates            | 108          |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 62.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 806          |\n",
      "|    ep_rew_mean          | 87.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 237          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 1999         |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052557993 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.1         |\n",
      "|    n_updates            | 112          |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    value_loss           | 45.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 817          |\n",
      "|    ep_rew_mean          | 86.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 2054         |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057048053 |\n",
      "|    clip_fraction        | 0.046        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.959        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.3         |\n",
      "|    n_updates            | 116          |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 38           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 860          |\n",
      "|    ep_rew_mean          | 84.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 2091         |\n",
      "|    total_timesteps      | 507904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051032295 |\n",
      "|    clip_fraction        | 0.0392       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.958        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.8         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    value_loss           | 36           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x17bea3190>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 958         |\n",
      "|    ep_rew_mean          | 112         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1615        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 415         |\n",
      "|    total_timesteps      | 671744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006402517 |\n",
      "|    clip_fraction        | 0.0849      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.73        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    value_loss           | 26.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 964         |\n",
      "|    ep_rew_mean          | 115         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1599        |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 430         |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005096845 |\n",
      "|    clip_fraction        | 0.0333      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.03        |\n",
      "|    n_updates            | 164         |\n",
      "|    policy_gradient_loss | 0.000652    |\n",
      "|    value_loss           | 30.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 970          |\n",
      "|    ep_rew_mean          | 121          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1589         |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 443          |\n",
      "|    total_timesteps      | 704512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050007417 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.08         |\n",
      "|    n_updates            | 168          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    value_loss           | 8.61         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 969          |\n",
      "|    ep_rew_mean          | 124          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1577         |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 457          |\n",
      "|    total_timesteps      | 720896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051765274 |\n",
      "|    clip_fraction        | 0.0405       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.994       |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.84         |\n",
      "|    n_updates            | 172          |\n",
      "|    policy_gradient_loss | -3.37e-05    |\n",
      "|    value_loss           | 8            |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 975         |\n",
      "|    ep_rew_mean          | 130         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1569        |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 469         |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004510527 |\n",
      "|    clip_fraction        | 0.0487      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.988      |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.95        |\n",
      "|    n_updates            | 176         |\n",
      "|    policy_gradient_loss | 0.000257    |\n",
      "|    value_loss           | 18          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 973          |\n",
      "|    ep_rew_mean          | 129          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1560         |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 482          |\n",
      "|    total_timesteps      | 753664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057018213 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.947       |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.64         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    value_loss           | 7.04         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 980          |\n",
      "|    ep_rew_mean          | 131          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1548         |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 497          |\n",
      "|    total_timesteps      | 770048       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041710096 |\n",
      "|    clip_fraction        | 0.0482       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.956       |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.89         |\n",
      "|    n_updates            | 184          |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 17.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 980          |\n",
      "|    ep_rew_mean          | 132          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1539         |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 510          |\n",
      "|    total_timesteps      | 786432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046044467 |\n",
      "|    clip_fraction        | 0.0315       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.92        |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.16         |\n",
      "|    n_updates            | 188          |\n",
      "|    policy_gradient_loss | 0.000141     |\n",
      "|    value_loss           | 9.59         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 973          |\n",
      "|    ep_rew_mean          | 129          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1529         |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 524          |\n",
      "|    total_timesteps      | 802816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041302657 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.915       |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.53         |\n",
      "|    n_updates            | 192          |\n",
      "|    policy_gradient_loss | 0.00015      |\n",
      "|    value_loss           | 3.61         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 979          |\n",
      "|    ep_rew_mean          | 130          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1522         |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 538          |\n",
      "|    total_timesteps      | 819200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041864957 |\n",
      "|    clip_fraction        | 0.0434       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.919       |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.8          |\n",
      "|    n_updates            | 196          |\n",
      "|    policy_gradient_loss | -0.000169    |\n",
      "|    value_loss           | 10.6         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 979         |\n",
      "|    ep_rew_mean          | 130         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1515        |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 551         |\n",
      "|    total_timesteps      | 835584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004787629 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.891      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.38        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00195    |\n",
      "|    value_loss           | 3.6         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 973         |\n",
      "|    ep_rew_mean          | 138         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1511        |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 563         |\n",
      "|    total_timesteps      | 851968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004737586 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.811      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.831       |\n",
      "|    n_updates            | 204         |\n",
      "|    policy_gradient_loss | -0.00164    |\n",
      "|    value_loss           | 1.96        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 955          |\n",
      "|    ep_rew_mean          | 145          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1510         |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 575          |\n",
      "|    total_timesteps      | 868352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047692424 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.792       |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 89.8         |\n",
      "|    n_updates            | 208          |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    value_loss           | 115          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 908         |\n",
      "|    ep_rew_mean          | 160         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1507        |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 586         |\n",
      "|    total_timesteps      | 884736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004010981 |\n",
      "|    clip_fraction        | 0.0308      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.751      |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.93        |\n",
      "|    n_updates            | 212         |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    value_loss           | 119         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 807          |\n",
      "|    ep_rew_mean          | 182          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1511         |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 595          |\n",
      "|    total_timesteps      | 901120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061382307 |\n",
      "|    clip_fraction        | 0.0694       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.72        |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.7         |\n",
      "|    n_updates            | 216          |\n",
      "|    policy_gradient_loss | -0.00304     |\n",
      "|    value_loss           | 215          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 666          |\n",
      "|    ep_rew_mean          | 203          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1518         |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 604          |\n",
      "|    total_timesteps      | 917504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047522644 |\n",
      "|    clip_fraction        | 0.0489       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.717       |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 205          |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    value_loss           | 296          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 514          |\n",
      "|    ep_rew_mean          | 219          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1525         |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 611          |\n",
      "|    total_timesteps      | 933888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075008348 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.734       |\n",
      "|    explained_variance   | 0.668        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 165          |\n",
      "|    n_updates            | 224          |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    value_loss           | 315          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 408        |\n",
      "|    ep_rew_mean          | 238        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1535       |\n",
      "|    iterations           | 58         |\n",
      "|    time_elapsed         | 619        |\n",
      "|    total_timesteps      | 950272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00816523 |\n",
      "|    clip_fraction        | 0.0867     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.827     |\n",
      "|    explained_variance   | 0.657      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 49.3       |\n",
      "|    n_updates            | 228        |\n",
      "|    policy_gradient_loss | -0.00313   |\n",
      "|    value_loss           | 213        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 367          |\n",
      "|    ep_rew_mean          | 247          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1544         |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 626          |\n",
      "|    total_timesteps      | 966656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056923777 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.78        |\n",
      "|    explained_variance   | 0.636        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 87.7         |\n",
      "|    n_updates            | 232          |\n",
      "|    policy_gradient_loss | -0.00099     |\n",
      "|    value_loss           | 200          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 357          |\n",
      "|    ep_rew_mean          | 242          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1552         |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 633          |\n",
      "|    total_timesteps      | 983040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042090993 |\n",
      "|    clip_fraction        | 0.0474       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.8         |\n",
      "|    explained_variance   | 0.741        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.7         |\n",
      "|    n_updates            | 236          |\n",
      "|    policy_gradient_loss | 0.000582     |\n",
      "|    value_loss           | 125          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 348          |\n",
      "|    ep_rew_mean          | 242          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1562         |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 639          |\n",
      "|    total_timesteps      | 999424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037236174 |\n",
      "|    clip_fraction        | 0.033        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.81        |\n",
      "|    explained_variance   | 0.474        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.6         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00022     |\n",
      "|    value_loss           | 234          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 343        |\n",
      "|    ep_rew_mean          | 249        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1572       |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 646        |\n",
      "|    total_timesteps      | 1015808    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00475253 |\n",
      "|    clip_fraction        | 0.0458     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.799     |\n",
      "|    explained_variance   | 0.713      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 21.9       |\n",
      "|    n_updates            | 244        |\n",
      "|    policy_gradient_loss | -9.92e-05  |\n",
      "|    value_loss           | 146        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1658d52e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar por 500,000 pasos de tiempo\n",
    "modelo.learn(total_timesteps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "nombre_modelo = \"../modelos/ppo-LunarLander-v2\"\n",
    "modelo.save(nombre_modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluar al agente\n",
    "\n",
    "- Ahora que nuestro agente Lunar Lander est√° entrenado, debemos **verificar su rendimiento**.\n",
    "- Stable-Baselines3 proporciona un m√©todo para hacerlo: `evaluate_policy`.\n",
    "- En el siguiente paso, veremos **c√≥mo evaluar y compartir autom√°ticamente a su agente para competir en una tabla de clasificaci√≥n, pero por ahora hag√°moslo nosotros mismos**\n",
    "\n",
    "\n",
    "Cuando eval√∫es a tu agente, no debes usar tu entorno de entrenamiento sino crear un entorno de evaluaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wladimir/mambaforge/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promedio recompensa = 248.00 +/- 13.526558114157996\n"
     ]
    }
   ],
   "source": [
    "#Evaluar el modelo\n",
    "modelo.load( \"../modelos/ppo-LunarLander-v2\")\n",
    "ambiente_evaluacion = gym.make(\"LunarLander-v2\")\n",
    "promedio_recompensa, desviacion_estandar_recompensa = evaluate_policy(modelo, ambiente_evaluacion, n_eval_episodes=10, deterministic=True, render=True)\n",
    "print(f\"promedio recompensa = {promedio_recompensa:.2f} +/- {desviacion_estandar_recompensa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ([[2.26493754e-01, 9.68281473e-02, 2.00996203e-01, 1.10319698e-01],\n",
    "       [3.80784492e-02, 4.09887606e-03, 1.93945411e-02, 1.84565503e-01],\n",
    "       [3.69621490e-02, 4.47967051e-02, 3.76096958e-02, 7.62097664e-02],\n",
    "       [3.11327746e-02, 1.38608398e-02, 3.98902721e-02, 5.47352183e-02],\n",
    "       [1.71422664e-01, 5.41767800e-02, 5.64236196e-02, 5.15562463e-02],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [5.76428864e-02, 1.19136825e-07, 2.13787318e-03, 3.74463995e-03],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [4.65385754e-02, 1.06708780e-01, 5.44332374e-02, 4.14419305e-01],\n",
    "       [4.69036428e-02, 7.37766391e-01, 9.51532052e-02, 1.21571473e-02],\n",
    "       [6.67931543e-01, 3.02143165e-02, 6.86178209e-03, 1.72628332e-03],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [1.37652509e-03, 5.27650906e-02, 8.27281626e-01, 3.67480120e-01],\n",
    "       [6.87458746e-01, 9.69657206e-01, 2.86161898e-01, 1.69681475e-01],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array(x).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ([[2.26493754e-01, 9.68281473e-02, 2.00996203e-01, 1.10319698e-01],\n",
    "       [3.80784492e-02, 4.09887606e-03, 1.93945411e-02, 1.84565503e-01],\n",
    "       [3.69621490e-02, 4.47967051e-02, 3.76096958e-02, 7.62097664e-02],\n",
    "       [3.11327746e-02, 1.38608398e-02, 3.98902721e-02, 5.47352183e-02],\n",
    "       [1.71422664e-01, 5.41767800e-02, 5.64236196e-02, 5.15562463e-02],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [5.76428864e-02, 1.19136825e-07, 2.13787318e-03, 3.74463995e-03],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [4.65385754e-02, 1.06708780e-01, 5.44332374e-02, 4.14419305e-01],\n",
    "       [4.69036428e-02, 7.37766391e-01, 9.51532052e-02, 1.21571473e-02],\n",
    "       [6.67931543e-01, 3.02143165e-02, 6.86178209e-03, 1.72628332e-03],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [1.37652509e-03, 5.27650906e-02, 8.27281626e-01, 3.67480120e-01],\n",
    "       [6.87458746e-01, 9.69657206e-01, 2.86161898e-01, 1.69681475e-01],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.26493754e-01, 9.68281473e-02, 2.00996203e-01, 1.10319698e-01],\n",
       "       [3.80784492e-02, 4.09887606e-03, 1.93945411e-02, 1.84565503e-01],\n",
       "       [3.69621490e-02, 4.47967051e-02, 3.76096958e-02, 7.62097664e-02],\n",
       "       [3.11327746e-02, 1.38608398e-02, 3.98902721e-02, 5.47352183e-02],\n",
       "       [1.71422664e-01, 5.41767800e-02, 5.64236196e-02, 5.15562463e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [5.76428864e-02, 1.19136825e-07, 2.13787318e-03, 3.74463995e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.65385754e-02, 1.06708780e-01, 5.44332374e-02, 4.14419305e-01],\n",
       "       [4.69036428e-02, 7.37766391e-01, 9.51532052e-02, 1.21571473e-02],\n",
       "       [6.67931543e-01, 3.02143165e-02, 6.86178209e-03, 1.72628332e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.37652509e-03, 5.27650906e-02, 8.27281626e-01, 3.67480120e-01],\n",
       "       [6.87458746e-01, 9.69657206e-01, 2.86161898e-01, 1.69681475e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.226, 0.097, 0.201, 0.11 ],\n",
       "       [0.038, 0.004, 0.019, 0.185],\n",
       "       [0.037, 0.045, 0.038, 0.076],\n",
       "       [0.031, 0.014, 0.04 , 0.055],\n",
       "       [0.171, 0.054, 0.056, 0.052],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.058, 0.   , 0.002, 0.004],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.047, 0.107, 0.054, 0.414],\n",
       "       [0.047, 0.738, 0.095, 0.012],\n",
       "       [0.668, 0.03 , 0.007, 0.002],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.001, 0.053, 0.827, 0.367],\n",
       "       [0.687, 0.97 , 0.286, 0.17 ],\n",
       "       [0.   , 0.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
