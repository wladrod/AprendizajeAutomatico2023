{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Tema 5: Aprendizaje por Refuerzo</h1>\n",
    "    <h1>Introducción al Aprendizaje por Refuerzo</h1>\n",
    "    <h1></h1>\n",
    "    <h5>Prof. Wladimir Rodriguez</h5>\n",
    "    <h5>wladimir@ula.ve</h5>\n",
    "    <h5>Departamento de Computación</h5>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por Refuerzo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "<center>\n",
    "<img src='../figuras/Agente_Entorno.png' width=\"50%\"/>\n",
    "</center>\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "El aprendizaje por refuerzo es aprender qué hacer, dada una situación y un conjunto de posibles acciones para elegir, con el fin de maximizar una recompensa. Al alumno, que llamaremos agente, no se le dice qué hacer, debe descubrirlo por sí mismo a través de la interacción con el ambienta. El objetivo es elegir sus acciones de tal manera que la recompensa acumulada se maximice. Entonces, elegir la mejor recompensa ahora, podría no ser la mejor decisión, a la larga.\n",
    "\n",
    "- Una definición formal\n",
    "\n",
    "> El aprendizaje por refuerzo es un marco para resolver tareas de control (también llamados problemas de decisión) mediante la creación de agentes que aprenden del ambiente interactuando con él a través de prueba y error y recibiendo recompensas (positivas o negativas) como su única retroalimentación.\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "<img src='../figuras/TiposAprendizaje.png' width=\"75%\"/>\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "El aprendizaje por refuerzo es diferente del aprendizaje supervisado, el tipo de aprendizaje estudiado\n",
    "en la mayoría de las aplicaciones en el campo del aprendizaje automático. El aprendizaje supervisado es\n",
    "aprender de un conjunto de entrenamiento de ejemplos etiquetados proporcionados por un supervisor externo.\n",
    "Cada ejemplo es una descripción de una situación junto con una especificación, la etiqueta, de la acción correcta que el sistema debería tomar en esa situación, que a menudo es identificar la categoría a la que pertenece la situación El objetivo de este tipo de aprendizaje es que el sistema extrapole o generalice sus respuestas para que actúe correctamente en situaciones que no están presentes en el conjunto de entrenamiento. Este es un tipo importante de aprendizaje, pero por si solo no es adecuado para aprender de la interacción. En problemas interactivos, a menudo no es práctico obtener ejemplos del comportamiento deseados que sean correctos y representativos de todas las situaciones en las que el agente tiene que actuar. En territorio inexplorado, donde uno esperaría que el aprendizaje sea más beneficioso, un agente debe ser capaz de aprender de su propia experiencia.\n",
    "\n",
    "El aprendizaje por refuerzo también es diferente del aprendizaje no supervisado, que generalmente trata de encontrar la estructura oculta en colecciones de datos sin etiqueta. Los términos aprendizaje supervisado y aprendizaje no supervisado parecerían clasificar exhaustivamente los paradigmas de aprendizaje automático, pero no es así. Aunque uno podría estar tentado a pensar en el aprendizaje por refuerzo como una especie de aprendizaje no supervisado porque no se basa en ejemplos de comportamiento correcto, el aprendizaje por refuerzo está intentando maximizar una señal de recompensa en lugar de tratar de encontrar la estructura oculta. Descubriendo la estructura en la experiencia de un agente ciertamente puede ser útil en el aprendizaje por refuerzo, pero por sí mismo no aborda el problema de aprendizaje por resfuerzo de maximizar la señal de recompensa.\n",
    "\n",
    "Por lo tanto, se puede considerar que el aprendizaje por refuerzo es un tercer paradigma de aprendizaje automático, junto con el aprendizaje supervisado y el aprendizaje no supervisado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de Aprendizaje por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Balance Carro-Poste\n",
    "\n",
    "<img src='../figuras/Car_Pole_Balancing.png' width='50%' />\n",
    "\n",
    "- **Objetivo**: equilibrar el poste sobre un carro en movimiento\n",
    "- **Estado**: ángulo, velocidad angular, posición, velocidad horizontal\n",
    "- **Acciones**: fuerza horizontal al carro\n",
    "- **Recompensa**: 1 en cada paso de tiempo si el poste está en posición vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Juegos de Atari\n",
    "\n",
    "#### Breakout\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"../figuras/atari.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "- **Objetivo**: Ganar el juego con el puntaje más alto\n",
    "- **Estado**: Píxeles de la pantalla del juego\n",
    "- **Acciones**: arriba, abajo, izquierda, derecha, etc.\n",
    "- **Recompensa** - Puntuación proporcionada por el juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pac-Mac\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"../figuras/PacMac.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "https://github.com/tychovdo/PacmanDQN\n",
    "\n",
    "- **Objetivo**: Ganar el juego con el puntaje más alto\n",
    "- **Estado**: Píxeles de la pantalla del juego\n",
    "- **Acciones**: arriba, abajo, izquierda, derecha, etc.\n",
    "- **Recompensa** - Puntuación proporcionada por el juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entrenar Robots para el embalaje \n",
    "\n",
    "<img src='../figuras/Robot.png' width=\"50%\" />\n",
    "\n",
    "- **Objetivo**: Elegir un dispositivo de una caja y ponerlo en un contenedor\n",
    "- **Estado**: Píxeles brutos del mundo real\n",
    "- **Acciones**: Posibles acciones del robot\n",
    "- **Recompensa**: Positiva al colocar un dispositivo con éxito; de lo contrario, negativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La hipótesis de la recompensa: la idea central del Aprendizaje por Refuerzo\n",
    "\n",
    "¿Por qué el objetivo del agente es maximizar el rendimiento esperado?\n",
    "\n",
    "Porque el Aprendizaje por Refuerzo se basa en la hipótesis de la recompensa, que es que todos los objetivos pueden describirse como la maximización del rendimiento esperado (recompensa acumulada esperada).\n",
    "\n",
    "Es por eso que en el Aprendizaje por Refuerzo, para tener el mejor comportamiento, necesitamos maximizar la recompensa acumulada esperada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalizando el problema del Aprendizaje por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Proceso de Decisión de Markov (MDP por sus siglas en inglés) es una formulación matemática del problema del Aprendizaje por Refuerzo. Que satisfacen la propiedad de Markov:\n",
    "\n",
    "**Propiedad de Markov**: el estado actual representa por completo el estado del ambiente (mundo). Es decir, el futuro depende solo del presente.\n",
    "\n",
    "Un MDP puede definirse por (S, A, R, P, γ) donde:\n",
    "\n",
    "- **S**: conjunto de posibles estados\n",
    "- **A**: conjunto de posibles acciones\n",
    "- **R**: distribución de la probabilidad de la recompensa dado el par (estado, acción)\n",
    "- **P**: distribución de probabilidad de que tan posible que alguno de los estados sea el nuevo estado, dado el par (estado, acción). También conocido como probabilidad de transición.\n",
    "- **$\\gamma$**: factor de descuento de la recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Espacio de Observaciones/Estados\n",
    "\n",
    "Las observaciones/estados son la información que nuestro agente obtiene del entorno. En el caso de un videojuego, puede ser un fotograma (una captura de pantalla). En el caso del agente comercial, puede ser el valor de una determinada acción, etc.\n",
    "\n",
    "Hay que hacer una diferenciación entre observación y estado:\n",
    "\n",
    "- Estado $s$: es una descripción completa del estado del mundo (no hay información oculta). En un entorno completamente observado.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/chess.jpg'/>\n",
    "</center>\n",
    "\n",
    "En el juego de ajedrez, recibimos un estado del entorno ya que tenemos acceso a toda la información del tablero. Por esto en un juego de ajedrez, estamos en un entorno completamente observado.\n",
    "\n",
    "- Observación $o$: es una descripción parcial del estado. En un entorno parcialmente observado.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/mario.jpg'/>\n",
    "</center>\n",
    "\n",
    "En Super Mario Bros, estamos en un entorno parcialmente observado, solo vemos una parte del nivel cerca del jugador, por lo que recibimos una observación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Espacio de acción\n",
    "\n",
    "El espacio de acción es el conjunto de todas las acciones posibles en un entorno. Las acciones pueden provenir de un espacio discreto o continuo:\n",
    "\n",
    "- *Espacio discreto*: el número de acciones posibles es finito.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/mario.jpg'/>\n",
    "</center>\n",
    "\n",
    "En Super Mario Bros, tenemos un conjunto finito de acciones ya que solo tenemos 4 direcciones y salto.\n",
    "\n",
    "- *Espacio continuo*: el número de acciones posibles es infinito.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/self_driving_car.jpeg'/>\n",
    "</center>\n",
    "\n",
    "Un agente de Carro Autónomo tiene infinidad de acciones posibles ya que puede girar 20° a la izquierda, 21,1°, 21,2°, tocar la bocina, girar 20° a la derecha…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompensas y descuentos\n",
    "\n",
    "La recompensa es fundamental en RL porque es la única retroalimentación para el agente. Gracias a ella, nuestro agente sabe si la acción realizada fue buena o no.\n",
    "\n",
    "La recompensa acumulada en cada paso de tiempo t se puede escribir como:\n",
    "\n",
    "$$R(\\tau)=\\sum_{t\\ge 0}^\\infty r_t$$\n",
    "\n",
    "$$R_t = r_t + r_{t+1} + \\dots + r_n$$\n",
    "\n",
    "Sin embargo, en realidad, no podemos simplemente agregarlos así. Las recompensas que llegan antes (al comienzo del juego) tienen más probabilidades de suceder, ya que son más predecibles que las recompensas futuras a largo plazo. Por lo que se descontaran las recompenzas a largo plazo \n",
    "\n",
    "Para descontar las recompensas, procedemos así:\n",
    "\n",
    "- Definimos una tasa de descuento llamada gamma ($\\gamma$). Debe estar entre 0 y 1. La mayoría de las veces entre 0,99 y 0,95.\n",
    "- Cuanto mayor sea la gamma, menor será el descuento. Esto significa que nuestro agente se preocupa más por la recompensa a largo plazo.\n",
    "- Por otro lado, cuanto menor sea la gamma, mayor será el descuento. Esto significa que nuestro agente se preocupa más por la recompensa a corto plazo.\n",
    "- Luego, cada recompensa será descontada por gamma al exponente del paso de tiempo, por lo que la recompensa futura es cada vez menos probable.\n",
    "\n",
    "$$R(\\tau)=\\sum_{t\\ge 0}^\\infty \\gamma^k r_t$$\n",
    "\n",
    "$$R_t = r_t + {\\gamma}r_{t+1} + \\dots + \\gamma^{n-t}r_n = r_t+{\\gamma}R_{t+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipo de tareas\n",
    "\n",
    "Una tarea es una instancia de un problema de aprendizaje por refuerzo. Podemos tener dos tipos de tareas: episódicas y continuas.\n",
    "\n",
    "\n",
    "- *Tarea episódica*: En este caso, tenemos un punto de partida y un punto final (un estado terminal). Esto crea un episodio: una lista de Estados, Acciones, Recompensas y nuevos Estados. Por ejemplo, piensa en Super Mario Bros: un episodio comienza con el lanzamiento de un nuevo nivel de Mario y termina cuando te matan o llegas al final del nivel.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/mario.jpg'/>\n",
    "</center>\n",
    "\n",
    "- *Tareas continuas*: Estas son tareas que continúan para siempre (sin estado terminal). En este caso, el agente debe aprender a elegir las mejores acciones y simultáneamente interactuar con el entorno. Por ejemplo, un agente que realiza transacciones bursátiles automatizadas. Para esta tarea, no hay un punto de partida ni un estado terminal. El agente sigue corriendo hasta que decidimos detenerlo.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/bolsa.png'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compromiso entre exploración/explotación\n",
    "\n",
    "Finalmente, antes de ver los diferentes métodos para resolver problemas de aprendizaje por refuerzo, debemos cubrir otro tema muy importante: la compensación de exploración/explotación.\n",
    "\n",
    "- La exploración es explorar el entorno al intentar acciones aleatorias para encontrar más información sobre el entorno.\n",
    "\n",
    "- La explotación es explotar información conocida para maximizar la recompensa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existen dos enfoques principales para resolver problemas de Aprendizaje por Refuerzo\n",
    "\n",
    "Ahora que aprendimos el enfoque de Aprendizaje por Refuerzo, ¿cómo resolvemos el problema de Aprendizaje por Refuerzo?\n",
    "\n",
    "En otros términos, ¿cómo construir un agente de Aprendizaje por Refuerzo que pueda seleccionar las acciones que maximicen su recompensa acumulada esperada?\n",
    "\n",
    "\n",
    "#### La Política $\\pi$: el cerebro del agente\n",
    "\n",
    "La Política $\\pi$ es el cerebro de nuestro Agente, es la función que nos dice qué acción tomar dado el estado en el que nos encontramos. Por lo que define el comportamiento del agente en un momento dado.\n",
    "\n",
    "Esta Política es la función que queremos aprender, nuestro objetivo es encontrar la política óptima $\\pi^*$, la política que maximiza la rentabilidad esperada cuando el agente actúa de acuerdo con ella. Encontramos esta $\\pi^*$ a través del entrenamiento.\n",
    "\n",
    "Hay dos enfoques para entrenar a nuestro agente para encontrar esta política óptima $\\pi^*$:\n",
    "\n",
    "- *Directamente*, enseñando al agente a aprender qué acción tomar, dado el estado en el que se encuentra: Métodos Basados en Políticas.\n",
    "- *Indirectamente*, enseñe al agente a aprender qué estado es más valioso y luego tome la acción que lo lleve a los estados más valiosos: Métodos Basados en Valores.\n",
    "\n",
    "#### Métodos Basados en Políticas.\n",
    "\n",
    "En los métodos basados en políticas, aprendemos una función de política directamente.\n",
    "\n",
    "Esta función mapeará desde cada estado a la mejor acción correspondiente en ese estado. O una distribución de probabilidad sobre el conjunto de acciones posibles en ese estado.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/policy_based.png'/>\n",
    "</center>\n",
    "\n",
    "Tenemos dos tipos de políticas:\n",
    "\n",
    "- *Determinista*: una política en un estado determinado siempre devolverá la misma acción.\n",
    "\n",
    "$$a=\\pi(s)$$\n",
    "\n",
    "- Estocástica: genera una distibución de probabilidad sobre las acciones.\n",
    "\n",
    "$$\\pi(a|s)=P[A|s]$$\n",
    "\n",
    "política(acciones | estado) = distribución de probabilidad sobre el conjunto de acciones dado el estado actual\n",
    "\n",
    "#### Métodos basados en valores\n",
    "\n",
    "En los métodos basados en valores, en lugar de entrenar una función de política, entrenamos una función de valor que asigna un estado al valor esperado de estar en ese estado.\n",
    "\n",
    "El valor de un estado es el rendimiento descontado esperado que el agente puede obtener si comienza en ese estado y luego actúa de acuerdo con nuestra política.\n",
    "\n",
    "\"Actuar de acuerdo con nuestra política\" simplemente significa que nuestra política es \"ir al estado con el valor más alto\".\n",
    "\n",
    "$$v_\\pi(s)=\\mathbb{E}_\\pi[R_{t+1}+R_{t+2}+R_{t+3}+\\dots|S_t=s]$$\n",
    "\n",
    "Aquí vemos que nuestra función de valor definió el valor para cada estado posible.\n",
    "\n",
    "<center>\n",
    "    <img src='../figuras/value_based.png'/>\n",
    "</center>\n",
    "\n",
    "Gracias a nuestra función de valor, en cada paso nuestra política seleccionará el estado con el mayor valor definido por la función de valor: -7, luego -6, luego -5 (y así sucesivamente) hasta alcanzar la meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de Aprendizaje por Refuerzo\n",
    "\n",
    "En este ejemplo utilizaremos una librería para crear ambientes para el Aprendizaje por Refuerzo llamada [Gym](https://www.gymlibrary.dev).\n",
    "Gym es una librería Python de código abierto para desarrollar y comparar algoritmos de aprendizaje por refuerzo al proporcionar una API estándar para comunicar algoritmos con entornos de aprendizaje, así como un conjunto estándar de entornos que cumplen con esa API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería que contiene nuestro ambiente (entorno) se llama Gym.\n",
    "\n",
    "La biblioteca de Gym proporciona dos cosas:\n",
    "\n",
    "- Una interfaz que te permite crear ambientes de Aprendizaje por Refuerzo.\n",
    "- Una colección de ambientes (gym-control, atari, box2D...).\n",
    "\n",
    "Con Gym:\n",
    "\n",
    "1. Creamos nuestro entorno usando `gym.make()`\n",
    "\n",
    "2. Restablecemos el entorno a su estado inicial con `observacion = env.reset()`\n",
    "\n",
    "En cada paso:\n",
    "\n",
    "3. Obtén una acción usando nuestro modelo (en nuestro ejemplo tomamos una acción aleatoria)\n",
    "\n",
    "4. Usando `env.step(action)`, realizamos esta acción en el entorno y obtenemos\n",
    "    - `observación`: El nuevo estado ($s_{t+1}$)\n",
    "    - `recompenza`: La recompensa que obtenemos tras ejecutar la acción\n",
    "    - `listo`: Indica si el episodio terminó\n",
    "    - `info`: Un diccionario que proporciona información adicional (depende del ambiente).\n",
    "\n",
    "Si el episodio ha terminado:\n",
    "- Restablecemos el entorno a su estado inicial con `observation = env.reset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acción tomada: 3\n",
      "Acción tomada: 2\n",
      "Acción tomada: 2\n",
      "Acción tomada: 1\n",
      "Acción tomada: 1\n",
      "Acción tomada: 2\n",
      "Acción tomada: 1\n",
      "Acción tomada: 2\n",
      "Acción tomada: 3\n",
      "Acción tomada: 3\n",
      "Acción tomada: 3\n",
      "Acción tomada: 0\n",
      "Acción tomada: 0\n",
      "Acción tomada: 3\n",
      "Acción tomada: 1\n",
      "Acción tomada: 3\n",
      "Acción tomada: 1\n",
      "Acción tomada: 1\n",
      "Acción tomada: 1\n",
      "Acción tomada: 2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Primero, creamos un ambiente(entorno) llamado LunarLander-v2\n",
    "ambiente = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Posteriormente reiniciamos este ambiente\n",
    "observacion = ambiente.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "  # Tomar una acción al azar\n",
    "  accion = ambiente.action_space.sample()\n",
    "  print(\"Acción tomada:\", accion)\n",
    "\n",
    "  # Ejecutar la acción en el ambiente y obtener\n",
    "  # el próximo estado, rencompensa, listo e información adicional\n",
    "  observacion, rencompensa, listo, truncado, info = ambiente.step(accion)\n",
    "  \n",
    "  # Si el juego finalizo (en nuestro caso, aterizamos, nos estrellamos o se acabo el tiempo)\n",
    "  if listo:\n",
    "      # Resetear el ambiente\n",
    "      print(\"Reiniciar el ambiente\")\n",
    "      observacion = ambiente.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El ambiente\n",
    "\n",
    "En este ejemplo vamos a entrenar a nuestro agente, un `Lunar Lander`, para aterrizar correctamente en la luna. Para hacer eso, el agente necesita aprender a adaptar su velocidad y posición (horizontal, vertical y angular) para aterrizar correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____ESPACIO DE OBSERVACIÓN_____ \n",
      "\n",
      "Forma del Espacio de Observación (8,)\n",
      "Observación aleatoria [-0.98635596  0.9278961  -3.3600903   0.5214729  -2.5375593   3.4410696\n",
      "  0.76094836  0.8141851 ]\n"
     ]
    }
   ],
   "source": [
    "# Crear ambiente con gym.make(\"<nombre_del_ambiente>\")\n",
    "ambiente = gym.make(\"LunarLander-v2\")\n",
    "ambiente.reset()\n",
    "print(\"_____ESPACIO DE OBSERVACIÓN_____ \\n\")\n",
    "print(\"Forma del Espacio de Observación\", ambiente.observation_space.shape)\n",
    "print(\"Observación aleatoria\", ambiente.observation_space.sample()) # Obtener una observación aleatoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos con `Forma del Espacio de Observación (8,)` que la observación es un vector de tamaño 8, donde cada valor contiene información diferente sobre el módulo de aterrizaje:\n",
    "- Coordenada horizontal de la plataforma (x)\n",
    "- Coordenada vertical de la plataforma (y)\n",
    "- Velocidad horizontal (x)\n",
    "- Velocidad vertical (y)\n",
    "- Ángulo\n",
    "- Velocidad angular\n",
    "- Si la pierna izquierda tiene punto de contacto tocó la tierra\n",
    "- Si la pierna derecha tiene punto de contacto tocó la tierra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ESPACIO_DE_ACCIÓN_____ \n",
      "\n",
      "Forma del Espacio de Acción 4\n",
      "Acción aleatoria 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ESPACIO_DE_ACCIÓN_____ \\n\")\n",
    "print(\"Forma del Espacio de Acción\", ambiente.action_space.n)\n",
    "print(\"Acción aleatoria\", ambiente.action_space.sample()) # Obtener una acción aleatoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El espacio de acción (el conjunto de acciones posibles que puede realizar el agente) es discreto con 4 acciones disponibles:\n",
    "\n",
    "- Hacer nada,\n",
    "- Dispara motor de orientación izquierda,\n",
    "- Dispara el motor principal,\n",
    "- Dispara motor de orientación derecha.\n",
    "\n",
    "Función de recompensa (la función que otorga una recompensa en cada paso de tiempo):\n",
    "\n",
    "- Moverse desde la parte superior de la pantalla hasta la plataforma de aterrizaje y la velocidad cero es de aproximadamente 100 a 140 puntos.\n",
    "- El motor principal de disparo es -0.3 cada cuadro\n",
    "- Cada contacto con el suelo de la pierna es +10 puntos\n",
    "- El episodio termina si el módulo de aterrizaje se estrella (adicional -100 puntos) o se detiene (+100 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiente vectorizado\n",
    "\n",
    "Creamos un ambiente vectorizado (método para apilar múltiples ambientes independientes en un solo ambiente) de 16 ambientes, de esta manera, tendremos experiencias más diversas durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el ambiente(entorno)\n",
    "ambiente = make_vec_env('LunarLander-v2', n_envs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el modelo\n",
    "\n",
    "- Ahora que estudiamos nuestro ambiente y entendimos el problema: **poder aterrizar correctamente el módulo de aterrizaje lunar en la plataforma de aterrizaje controlando los motores de orientación izquierdo, derecho y principal**. Construyamos el algoritmo que vamos a usar para resolver este problema 🚀.\n",
    "\n",
    "- Para hacerlo, vamos a utilizar la librería de Aprendizaje por Refuerzo, [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/).\n",
    "\n",
    "- SB3 es un conjunto de **implementaciones confiables de algoritmos de aprendizaje por refuerzo en PyTorch**.\n",
    "\n",
    "Para resolver este problema, vamos a utilizar SB3 **PPO**. [PPO (también conocido como optimización de política proximal)](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).\n",
    "\n",
    "PPO es una combinación de:\n",
    "- *Método de aprendizaje por refuerzo basado en valores*: aprender una función de acción-valor que nos indicará cuál es la **acción más valiosa a realizar dado un estado y una acción**.\n",
    "- *Método de aprendizaje por refuerzo basado en políticas*: aprender una política que **nos dará una distribución de probabilidad sobre las acciones**.\n",
    "\n",
    "Stable-Baselines3 es fácil de configurar:\n",
    "\n",
    "1. Crear el ambiente (en nuestro caso se hizo arriba)\n",
    "\n",
    "2. Definir el modelo que se quiere usar y crear una instancia de este modelo\n",
    "\n",
    "3. Entrenar al agente con `modelo.learn` y defines el número de pasos de tiempo de entrenamiento\n",
    "\n",
    "```\n",
    "# Crear ambiente\n",
    "ambiente = gym.make('LunarLander-v2')\n",
    "\n",
    "# Instanciar el agente\n",
    "modelo = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Entrenar al agente\n",
    "modelo.learn(total_timesteps=int(2e5))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Crear modelo y seleccionar parámetros para accelerar el entrenamiento\n",
    "modelo = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = ambiente,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 64,\n",
    "    n_epochs = 4,\n",
    "    gamma = 0.999,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1,\n",
    "    device='mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar al modelo (agente) de PPO 🏃\n",
    "\n",
    "Entrenaremos a nuestro modelo para 500 000 intervalos de tiempo, no olvide usar GPU en Colab. Tomará aproximadamente ~10 minutos, pero puede usar menos intervalos de tiempo si solo desea probarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.5     |\n",
      "|    ep_rew_mean     | -187     |\n",
      "| time/              |          |\n",
      "|    fps             | 2265     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 91.4        |\n",
      "|    ep_rew_mean          | -166        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 954         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007501182 |\n",
      "|    clip_fraction        | 0.0674      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.00145    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.71e+03    |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.00559    |\n",
      "|    value_loss           | 5.08e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 93           |\n",
      "|    ep_rew_mean          | -136         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 804          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054419376 |\n",
      "|    clip_fraction        | 0.0421       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.00525     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.54e+03     |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | -0.00532     |\n",
      "|    value_loss           | 2.87e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 96           |\n",
      "|    ep_rew_mean          | -121         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 746          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067481156 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.001       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 449          |\n",
      "|    n_updates            | 12           |\n",
      "|    policy_gradient_loss | -0.00301     |\n",
      "|    value_loss           | 1.4e+03      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 94.8        |\n",
      "|    ep_rew_mean          | -97.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 715         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007519068 |\n",
      "|    clip_fraction        | 0.0885      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.000106   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 326         |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | -0.00596    |\n",
      "|    value_loss           | 590         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 106          |\n",
      "|    ep_rew_mean          | -90.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 695          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 141          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076959846 |\n",
      "|    clip_fraction        | 0.057        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | -0.000689    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 155          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00599     |\n",
      "|    value_loss           | 479          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 118         |\n",
      "|    ep_rew_mean          | -67.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 682         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010581596 |\n",
      "|    clip_fraction        | 0.0952      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | -4.53e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 287         |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.00765    |\n",
      "|    value_loss           | 565         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 117          |\n",
      "|    ep_rew_mean          | -52.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 673          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 194          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074475696 |\n",
      "|    clip_fraction        | 0.0594       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | -4.01e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 296          |\n",
      "|    n_updates            | 28           |\n",
      "|    policy_gradient_loss | -0.00416     |\n",
      "|    value_loss           | 453          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 126         |\n",
      "|    ep_rew_mean          | -47.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 511         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 288         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010510441 |\n",
      "|    clip_fraction        | 0.063       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | -2.38e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 265         |\n",
      "|    n_updates            | 32          |\n",
      "|    policy_gradient_loss | -0.00518    |\n",
      "|    value_loss           | 402         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 136        |\n",
      "|    ep_rew_mean          | -27.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 520        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 314        |\n",
      "|    total_timesteps      | 163840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00599605 |\n",
      "|    clip_fraction        | 0.0363     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.21      |\n",
      "|    explained_variance   | -1.79e-06  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 151        |\n",
      "|    n_updates            | 36         |\n",
      "|    policy_gradient_loss | -0.00192   |\n",
      "|    value_loss           | 404        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 134         |\n",
      "|    ep_rew_mean          | -22.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 527         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 341         |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007691025 |\n",
      "|    clip_fraction        | 0.033       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | -1.31e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 278         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00252    |\n",
      "|    value_loss           | 447         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 149         |\n",
      "|    ep_rew_mean          | -12.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 531         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 369         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007387871 |\n",
      "|    clip_fraction        | 0.032       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | -1.91e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 384         |\n",
      "|    n_updates            | 44          |\n",
      "|    policy_gradient_loss | -0.00173    |\n",
      "|    value_loss           | 561         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 184         |\n",
      "|    ep_rew_mean          | -8.27       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 532         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 400         |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005838329 |\n",
      "|    clip_fraction        | 0.0327      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 4.77e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 189         |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    value_loss           | 540         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 251         |\n",
      "|    ep_rew_mean          | -12.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 628         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008792279 |\n",
      "|    clip_fraction        | 0.0783      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | -4.77e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 341         |\n",
      "|    n_updates            | 52          |\n",
      "|    policy_gradient_loss | -0.00398    |\n",
      "|    value_loss           | 568         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 268         |\n",
      "|    ep_rew_mean          | -10.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 658         |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003560141 |\n",
      "|    clip_fraction        | 0.0252      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.000184    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 411         |\n",
      "|    n_updates            | 56          |\n",
      "|    policy_gradient_loss | -0.000927   |\n",
      "|    value_loss           | 578         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 318         |\n",
      "|    ep_rew_mean          | -12.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 702         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004570827 |\n",
      "|    clip_fraction        | 0.0252      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.00113     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 353         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0015     |\n",
      "|    value_loss           | 670         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 389          |\n",
      "|    ep_rew_mean          | -14.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 377          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 738          |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051891347 |\n",
      "|    clip_fraction        | 0.029        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.17        |\n",
      "|    explained_variance   | 0.0143       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 152          |\n",
      "|    n_updates            | 64           |\n",
      "|    policy_gradient_loss | -0.00278     |\n",
      "|    value_loss           | 420          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 490         |\n",
      "|    ep_rew_mean          | -10.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 774         |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004215752 |\n",
      "|    clip_fraction        | 0.014       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 70.3        |\n",
      "|    n_updates            | 68          |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 381         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 582         |\n",
      "|    ep_rew_mean          | -7.01       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 810         |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004452089 |\n",
      "|    clip_fraction        | 0.0265      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.546       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 122         |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.00252    |\n",
      "|    value_loss           | 238         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 619          |\n",
      "|    ep_rew_mean          | 4.88         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 385          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 850          |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055597886 |\n",
      "|    clip_fraction        | 0.0298       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.712        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 105          |\n",
      "|    n_updates            | 76           |\n",
      "|    policy_gradient_loss | -0.00278     |\n",
      "|    value_loss           | 174          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 680         |\n",
      "|    ep_rew_mean          | 20.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 326         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 1052        |\n",
      "|    total_timesteps      | 344064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006942802 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.4        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.003      |\n",
      "|    value_loss           | 114         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 706          |\n",
      "|    ep_rew_mean          | 37.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 209          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 1723         |\n",
      "|    total_timesteps      | 360448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066503263 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 66.6         |\n",
      "|    n_updates            | 84           |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 83.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 681         |\n",
      "|    ep_rew_mean          | 44.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 1758        |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006989425 |\n",
      "|    clip_fraction        | 0.0658      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59.6        |\n",
      "|    n_updates            | 88          |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    value_loss           | 163         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 653         |\n",
      "|    ep_rew_mean          | 49.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 219         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 1794        |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004777539 |\n",
      "|    clip_fraction        | 0.0295      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59.5        |\n",
      "|    n_updates            | 92          |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    value_loss           | 173         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 667         |\n",
      "|    ep_rew_mean          | 54.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 1836        |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006286043 |\n",
      "|    clip_fraction        | 0.0425      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.8        |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | -0.00221    |\n",
      "|    value_loss           | 130         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 685         |\n",
      "|    ep_rew_mean          | 61.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 1874        |\n",
      "|    total_timesteps      | 425984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004469488 |\n",
      "|    clip_fraction        | 0.034       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.3        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.000971   |\n",
      "|    value_loss           | 63.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 723          |\n",
      "|    ep_rew_mean          | 71.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 231          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 1909         |\n",
      "|    total_timesteps      | 442368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056932936 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.6         |\n",
      "|    n_updates            | 104          |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 759          |\n",
      "|    ep_rew_mean          | 81.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 235          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 1946         |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051294314 |\n",
      "|    clip_fraction        | 0.0357       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.62         |\n",
      "|    n_updates            | 108          |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 62.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 806          |\n",
      "|    ep_rew_mean          | 87.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 237          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 1999         |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052557993 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.1         |\n",
      "|    n_updates            | 112          |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    value_loss           | 45.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 817          |\n",
      "|    ep_rew_mean          | 86.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 2054         |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057048053 |\n",
      "|    clip_fraction        | 0.046        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.959        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.3         |\n",
      "|    n_updates            | 116          |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 38           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 860          |\n",
      "|    ep_rew_mean          | 84.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 2091         |\n",
      "|    total_timesteps      | 507904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051032295 |\n",
      "|    clip_fraction        | 0.0392       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.04        |\n",
      "|    explained_variance   | 0.958        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.8         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    value_loss           | 36           |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x17bea3190>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 958         |\n",
      "|    ep_rew_mean          | 112         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1615        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 415         |\n",
      "|    total_timesteps      | 671744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006402517 |\n",
      "|    clip_fraction        | 0.0849      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.73        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    value_loss           | 26.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 964         |\n",
      "|    ep_rew_mean          | 115         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1599        |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 430         |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005096845 |\n",
      "|    clip_fraction        | 0.0333      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.03        |\n",
      "|    n_updates            | 164         |\n",
      "|    policy_gradient_loss | 0.000652    |\n",
      "|    value_loss           | 30.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 970          |\n",
      "|    ep_rew_mean          | 121          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1589         |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 443          |\n",
      "|    total_timesteps      | 704512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050007417 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.08         |\n",
      "|    n_updates            | 168          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    value_loss           | 8.61         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 969          |\n",
      "|    ep_rew_mean          | 124          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1577         |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 457          |\n",
      "|    total_timesteps      | 720896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051765274 |\n",
      "|    clip_fraction        | 0.0405       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.994       |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.84         |\n",
      "|    n_updates            | 172          |\n",
      "|    policy_gradient_loss | -3.37e-05    |\n",
      "|    value_loss           | 8            |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 975         |\n",
      "|    ep_rew_mean          | 130         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1569        |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 469         |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004510527 |\n",
      "|    clip_fraction        | 0.0487      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.988      |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.95        |\n",
      "|    n_updates            | 176         |\n",
      "|    policy_gradient_loss | 0.000257    |\n",
      "|    value_loss           | 18          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 973          |\n",
      "|    ep_rew_mean          | 129          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1560         |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 482          |\n",
      "|    total_timesteps      | 753664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057018213 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.947       |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.64         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    value_loss           | 7.04         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 980          |\n",
      "|    ep_rew_mean          | 131          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1548         |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 497          |\n",
      "|    total_timesteps      | 770048       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041710096 |\n",
      "|    clip_fraction        | 0.0482       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.956       |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.89         |\n",
      "|    n_updates            | 184          |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 17.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 980          |\n",
      "|    ep_rew_mean          | 132          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1539         |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 510          |\n",
      "|    total_timesteps      | 786432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046044467 |\n",
      "|    clip_fraction        | 0.0315       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.92        |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.16         |\n",
      "|    n_updates            | 188          |\n",
      "|    policy_gradient_loss | 0.000141     |\n",
      "|    value_loss           | 9.59         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 973          |\n",
      "|    ep_rew_mean          | 129          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1529         |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 524          |\n",
      "|    total_timesteps      | 802816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041302657 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.915       |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.53         |\n",
      "|    n_updates            | 192          |\n",
      "|    policy_gradient_loss | 0.00015      |\n",
      "|    value_loss           | 3.61         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 979          |\n",
      "|    ep_rew_mean          | 130          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1522         |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 538          |\n",
      "|    total_timesteps      | 819200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041864957 |\n",
      "|    clip_fraction        | 0.0434       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.919       |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.8          |\n",
      "|    n_updates            | 196          |\n",
      "|    policy_gradient_loss | -0.000169    |\n",
      "|    value_loss           | 10.6         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 979         |\n",
      "|    ep_rew_mean          | 130         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1515        |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 551         |\n",
      "|    total_timesteps      | 835584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004787629 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.891      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.38        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00195    |\n",
      "|    value_loss           | 3.6         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 973         |\n",
      "|    ep_rew_mean          | 138         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1511        |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 563         |\n",
      "|    total_timesteps      | 851968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004737586 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.811      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.831       |\n",
      "|    n_updates            | 204         |\n",
      "|    policy_gradient_loss | -0.00164    |\n",
      "|    value_loss           | 1.96        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 955          |\n",
      "|    ep_rew_mean          | 145          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1510         |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 575          |\n",
      "|    total_timesteps      | 868352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047692424 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.792       |\n",
      "|    explained_variance   | 0.917        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 89.8         |\n",
      "|    n_updates            | 208          |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    value_loss           | 115          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 908         |\n",
      "|    ep_rew_mean          | 160         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1507        |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 586         |\n",
      "|    total_timesteps      | 884736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004010981 |\n",
      "|    clip_fraction        | 0.0308      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.751      |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.93        |\n",
      "|    n_updates            | 212         |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    value_loss           | 119         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 807          |\n",
      "|    ep_rew_mean          | 182          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1511         |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 595          |\n",
      "|    total_timesteps      | 901120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061382307 |\n",
      "|    clip_fraction        | 0.0694       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.72        |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.7         |\n",
      "|    n_updates            | 216          |\n",
      "|    policy_gradient_loss | -0.00304     |\n",
      "|    value_loss           | 215          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 666          |\n",
      "|    ep_rew_mean          | 203          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1518         |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 604          |\n",
      "|    total_timesteps      | 917504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047522644 |\n",
      "|    clip_fraction        | 0.0489       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.717       |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 205          |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    value_loss           | 296          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 514          |\n",
      "|    ep_rew_mean          | 219          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1525         |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 611          |\n",
      "|    total_timesteps      | 933888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075008348 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.734       |\n",
      "|    explained_variance   | 0.668        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 165          |\n",
      "|    n_updates            | 224          |\n",
      "|    policy_gradient_loss | -0.00283     |\n",
      "|    value_loss           | 315          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 408        |\n",
      "|    ep_rew_mean          | 238        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1535       |\n",
      "|    iterations           | 58         |\n",
      "|    time_elapsed         | 619        |\n",
      "|    total_timesteps      | 950272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00816523 |\n",
      "|    clip_fraction        | 0.0867     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.827     |\n",
      "|    explained_variance   | 0.657      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 49.3       |\n",
      "|    n_updates            | 228        |\n",
      "|    policy_gradient_loss | -0.00313   |\n",
      "|    value_loss           | 213        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 367          |\n",
      "|    ep_rew_mean          | 247          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1544         |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 626          |\n",
      "|    total_timesteps      | 966656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056923777 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.78        |\n",
      "|    explained_variance   | 0.636        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 87.7         |\n",
      "|    n_updates            | 232          |\n",
      "|    policy_gradient_loss | -0.00099     |\n",
      "|    value_loss           | 200          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 357          |\n",
      "|    ep_rew_mean          | 242          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1552         |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 633          |\n",
      "|    total_timesteps      | 983040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042090993 |\n",
      "|    clip_fraction        | 0.0474       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.8         |\n",
      "|    explained_variance   | 0.741        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.7         |\n",
      "|    n_updates            | 236          |\n",
      "|    policy_gradient_loss | 0.000582     |\n",
      "|    value_loss           | 125          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 348          |\n",
      "|    ep_rew_mean          | 242          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1562         |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 639          |\n",
      "|    total_timesteps      | 999424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037236174 |\n",
      "|    clip_fraction        | 0.033        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.81        |\n",
      "|    explained_variance   | 0.474        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.6         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00022     |\n",
      "|    value_loss           | 234          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 343        |\n",
      "|    ep_rew_mean          | 249        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1572       |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 646        |\n",
      "|    total_timesteps      | 1015808    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00475253 |\n",
      "|    clip_fraction        | 0.0458     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.799     |\n",
      "|    explained_variance   | 0.713      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 21.9       |\n",
      "|    n_updates            | 244        |\n",
      "|    policy_gradient_loss | -9.92e-05  |\n",
      "|    value_loss           | 146        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1658d52e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar por 500,000 pasos de tiempo\n",
    "modelo.learn(total_timesteps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "nombre_modelo = \"../modelos/ppo-LunarLander-v2\"\n",
    "modelo.save(nombre_modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluar al agente\n",
    "\n",
    "- Ahora que nuestro agente Lunar Lander está entrenado, debemos **verificar su rendimiento**.\n",
    "- Stable-Baselines3 proporciona un método para hacerlo: `evaluate_policy`.\n",
    "- En el siguiente paso, veremos **cómo evaluar y compartir automáticamente a su agente para competir en una tabla de clasificación, pero por ahora hagámoslo nosotros mismos**\n",
    "\n",
    "\n",
    "Cuando evalúes a tu agente, no debes usar tu entorno de entrenamiento sino crear un entorno de evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wladimir/mambaforge/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promedio recompensa = 248.00 +/- 13.526558114157996\n"
     ]
    }
   ],
   "source": [
    "#Evaluar el modelo\n",
    "modelo.load( \"../modelos/ppo-LunarLander-v2\")\n",
    "ambiente_evaluacion = gym.make(\"LunarLander-v2\")\n",
    "promedio_recompensa, desviacion_estandar_recompensa = evaluate_policy(modelo, ambiente_evaluacion, n_eval_episodes=10, deterministic=True, render=True)\n",
    "print(f\"promedio recompensa = {promedio_recompensa:.2f} +/- {desviacion_estandar_recompensa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ([[2.26493754e-01, 9.68281473e-02, 2.00996203e-01, 1.10319698e-01],\n",
    "       [3.80784492e-02, 4.09887606e-03, 1.93945411e-02, 1.84565503e-01],\n",
    "       [3.69621490e-02, 4.47967051e-02, 3.76096958e-02, 7.62097664e-02],\n",
    "       [3.11327746e-02, 1.38608398e-02, 3.98902721e-02, 5.47352183e-02],\n",
    "       [1.71422664e-01, 5.41767800e-02, 5.64236196e-02, 5.15562463e-02],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [5.76428864e-02, 1.19136825e-07, 2.13787318e-03, 3.74463995e-03],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [4.65385754e-02, 1.06708780e-01, 5.44332374e-02, 4.14419305e-01],\n",
    "       [4.69036428e-02, 7.37766391e-01, 9.51532052e-02, 1.21571473e-02],\n",
    "       [6.67931543e-01, 3.02143165e-02, 6.86178209e-03, 1.72628332e-03],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [1.37652509e-03, 5.27650906e-02, 8.27281626e-01, 3.67480120e-01],\n",
    "       [6.87458746e-01, 9.69657206e-01, 2.86161898e-01, 1.69681475e-01],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array(x).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ([[2.26493754e-01, 9.68281473e-02, 2.00996203e-01, 1.10319698e-01],\n",
    "       [3.80784492e-02, 4.09887606e-03, 1.93945411e-02, 1.84565503e-01],\n",
    "       [3.69621490e-02, 4.47967051e-02, 3.76096958e-02, 7.62097664e-02],\n",
    "       [3.11327746e-02, 1.38608398e-02, 3.98902721e-02, 5.47352183e-02],\n",
    "       [1.71422664e-01, 5.41767800e-02, 5.64236196e-02, 5.15562463e-02],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [5.76428864e-02, 1.19136825e-07, 2.13787318e-03, 3.74463995e-03],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [4.65385754e-02, 1.06708780e-01, 5.44332374e-02, 4.14419305e-01],\n",
    "       [4.69036428e-02, 7.37766391e-01, 9.51532052e-02, 1.21571473e-02],\n",
    "       [6.67931543e-01, 3.02143165e-02, 6.86178209e-03, 1.72628332e-03],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
    "       [1.37652509e-03, 5.27650906e-02, 8.27281626e-01, 3.67480120e-01],\n",
    "       [6.87458746e-01, 9.69657206e-01, 2.86161898e-01, 1.69681475e-01],\n",
    "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.26493754e-01, 9.68281473e-02, 2.00996203e-01, 1.10319698e-01],\n",
       "       [3.80784492e-02, 4.09887606e-03, 1.93945411e-02, 1.84565503e-01],\n",
       "       [3.69621490e-02, 4.47967051e-02, 3.76096958e-02, 7.62097664e-02],\n",
       "       [3.11327746e-02, 1.38608398e-02, 3.98902721e-02, 5.47352183e-02],\n",
       "       [1.71422664e-01, 5.41767800e-02, 5.64236196e-02, 5.15562463e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [5.76428864e-02, 1.19136825e-07, 2.13787318e-03, 3.74463995e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.65385754e-02, 1.06708780e-01, 5.44332374e-02, 4.14419305e-01],\n",
       "       [4.69036428e-02, 7.37766391e-01, 9.51532052e-02, 1.21571473e-02],\n",
       "       [6.67931543e-01, 3.02143165e-02, 6.86178209e-03, 1.72628332e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.37652509e-03, 5.27650906e-02, 8.27281626e-01, 3.67480120e-01],\n",
       "       [6.87458746e-01, 9.69657206e-01, 2.86161898e-01, 1.69681475e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.226, 0.097, 0.201, 0.11 ],\n",
       "       [0.038, 0.004, 0.019, 0.185],\n",
       "       [0.037, 0.045, 0.038, 0.076],\n",
       "       [0.031, 0.014, 0.04 , 0.055],\n",
       "       [0.171, 0.054, 0.056, 0.052],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.058, 0.   , 0.002, 0.004],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.047, 0.107, 0.054, 0.414],\n",
       "       [0.047, 0.738, 0.095, 0.012],\n",
       "       [0.668, 0.03 , 0.007, 0.002],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.001, 0.053, 0.827, 0.367],\n",
       "       [0.687, 0.97 , 0.286, 0.17 ],\n",
       "       [0.   , 0.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
